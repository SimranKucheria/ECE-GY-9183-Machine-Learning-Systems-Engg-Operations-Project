name: deeptrust_serve
services:
  fastapi_server:
    build:
      context: ./inference_service  # relative path to FastAPI code
      dockerfile: Dockerfile
    container_name: fastapi_server
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    # ports:
    #   - "8000:8000"  # for HTTP requests

  triton_server:
    build:
      # context: /home/cc/work/BLIP
      context: ./BLIP  # relative path to Triton code
      dockerfile: Dockerfile.triton
    container_name: triton_server
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    # ports:
    #   - "8000:8000"  # for HTTP requests
    #   - "8001:8001"  # for GRPC requests
    #   - "8002:8002"  # for reporting metrics

  vllm:
    image: vllm/vllm-openai:latest
    container_name: vllm
    runtime: nvidia
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HF_API_KEY}
      - NVIDIA_VISIBLE_DEVICES=all
    # ports:
    #   - "8005:8000"
    volumes:
      - ~/.cache/huggingface:/root/.cache/huggingface
    command: --model --model mistralai/Mistral-7B-Instruct-v0.2 --dtype float16
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              capabilities: [gpu]
    ipc: host

  jupyter:
    image: quay.io/jupyter/minimal-notebook:latest
    container_name: jupyter
    # ports:
    #   - "8888:8888"
    volumes:
      - /home/cc/deeptrust:/home/jovyan/work
      - /mnt/object:/mnt/data
    command: >
      bash -c "python3 -m pip install numpy && start-notebook.sh"


  fetcher:
    build:
      context: ./fetcher
      dockerfile: Dockerfile
    # ports:
    #   - "5000:5000"
    volumes:
      - ./inference_service/:/data/vit
      - ./BLIP/:/data/blip 
    restart: always

