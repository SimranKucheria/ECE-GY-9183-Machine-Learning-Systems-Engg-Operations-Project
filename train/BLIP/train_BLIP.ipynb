{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee7cd9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision.datasets.utils import download_url\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "494f9d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_caption(caption,max_words=50):\n",
    "    caption = re.sub(\n",
    "        r\"([.!\\\"()*#:;~])\",       \n",
    "        ' ',\n",
    "        caption.lower(),\n",
    "    )\n",
    "    caption = re.sub(\n",
    "        r\"\\s{2,}\",\n",
    "        ' ',\n",
    "        caption,\n",
    "    )\n",
    "    caption = caption.rstrip('\\n') \n",
    "    caption = caption.strip(' ')\n",
    "\n",
    "    #truncate caption\n",
    "    caption_words = caption.split(' ')\n",
    "    if len(caption_words)>max_words:\n",
    "        caption = ' '.join(caption_words[:max_words])\n",
    "            \n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f79ae2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class flickr30k_train_caption(Dataset):\n",
    "    def __init__(self, transform, image_root, ann_root, max_words=30, prompt=''):        \n",
    "        '''\n",
    "        image_root (string): Root directory of images (e.g. flickr30k/)\n",
    "        ann_root (string): directory to store the annotation file\n",
    "        '''        \n",
    "        url = 'https://storage.googleapis.com/sfr-vision-language-research/datasets/flickr30k_train.json'\n",
    "        filename = 'flickr30k_train.json'\n",
    "\n",
    "        download_url(url,ann_root)\n",
    "        \n",
    "        self.annotation = json.load(open(os.path.join(ann_root,filename),'r'))\n",
    "        self.transform = transform\n",
    "        self.image_root = image_root\n",
    "        self.max_words = max_words      \n",
    "        self.prompt = prompt\n",
    "        \n",
    "        self.img_ids = {}  \n",
    "        n = 0\n",
    "        for ann in self.annotation:\n",
    "            img_id = ann['image_id']\n",
    "            if img_id not in self.img_ids.keys():\n",
    "                self.img_ids[img_id] = n\n",
    "                n += 1    \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.annotation)\n",
    "    \n",
    "    def __getitem__(self, index):    \n",
    "        \n",
    "        ann = self.annotation[index]\n",
    "        \n",
    "        image_path = os.path.join(self.image_root,ann['image'])        \n",
    "        image = Image.open(image_path).convert('RGB')   \n",
    "        image = self.transform(image)\n",
    "        \n",
    "        caption = self.prompt+pre_caption(ann['caption'], self.max_words) \n",
    "\n",
    "        return image, caption, self.img_ids[ann['image_id']]\n",
    "    \n",
    "class flickr30k_eval_caption(Dataset):\n",
    "    def __init__(self, transform, image_root, ann_root, split):  \n",
    "        '''\n",
    "        image_root (string): Root directory of images (e.g. flickr30k/)\n",
    "        ann_root (string): directory to store the annotation file\n",
    "        split (string): val or test\n",
    "        '''\n",
    "        urls = {'val':'https://storage.googleapis.com/sfr-vision-language-research/datasets/flickr30k_val.json',\n",
    "                'test':'https://storage.googleapis.com/sfr-vision-language-research/datasets/flickr30k_test.json'}\n",
    "        filenames = {'val':'flickr30k_val.json','test':'flickr30k_test.json'}\n",
    "        \n",
    "        download_url(urls[split],ann_root)\n",
    "        \n",
    "        self.annotation = json.load(open(os.path.join(ann_root,filenames[split]),'r'))\n",
    "        self.transform = transform\n",
    "        self.image_root = image_root\n",
    "        \n",
    "                                    \n",
    "    def __len__(self):\n",
    "        return len(self.annotation)\n",
    "    \n",
    "    def __getitem__(self, index):    \n",
    "        ann = self.annotation[index]\n",
    "\n",
    "        image_path = os.path.join(self.image_root, ann['image'])        \n",
    "        image = Image.open(image_path).convert('RGB')    \n",
    "        image = self.transform(image)  \n",
    "\n",
    "        img_id = ann['image'].split('/')[-1].strip('.jpg').split('_')[-1]\n",
    "\n",
    "        return image, img_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef592d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1ec9f5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/timm/models/registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/timm/models/helpers.py:7: FutureWarning: Importing from timm.models.helpers is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import ruamel.yaml as yaml\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.distributed as dist\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "\n",
    "from timm.models.vision_transformer import _cfg, PatchEmbed\n",
    "from timm.models.registry import register_model\n",
    "from timm.models.layers import trunc_normal_, DropPath\n",
    "from timm.models.helpers import named_apply, adapt_input_conv\n",
    "\n",
    "from fairscale.nn.checkpoint.checkpoint_activations import checkpoint_wrapper\n",
    "\n",
    "import math\n",
    "import os\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from torch import Tensor, device, dtype, nn\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers.activations import ACT2FN\n",
    "from transformers.file_utils import (\n",
    "    ModelOutput,\n",
    ")\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutputWithPastAndCrossAttentions,\n",
    "    BaseModelOutputWithPoolingAndCrossAttentions,\n",
    "    CausalLMOutputWithCrossAttentions,\n",
    "    MaskedLMOutput,\n",
    "    MultipleChoiceModelOutput,\n",
    "    NextSentencePredictorOutput,\n",
    "    QuestionAnsweringModelOutput,\n",
    "    SequenceClassifierOutput,\n",
    "    TokenClassifierOutput,\n",
    ")\n",
    "from transformers.modeling_utils import (\n",
    "    PreTrainedModel,\n",
    "    apply_chunking_to_forward,\n",
    "    find_pruneable_heads_and_indices,\n",
    "    prune_linear_layer,\n",
    ")\n",
    "from transformers.utils import logging\n",
    "from transformers.models.bert.configuration_bert import BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea89ae43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp(nn.Module):\n",
    "    \"\"\" MLP as used in Vision Transformer, MLP-Mixer and related networks\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        # NOTE scale factor was wrong in my original version, can set manually to be compat with prev weights\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        self.attn_gradients = None\n",
    "        self.attention_map = None\n",
    "        \n",
    "    def save_attn_gradients(self, attn_gradients):\n",
    "        self.attn_gradients = attn_gradients\n",
    "        \n",
    "    def get_attn_gradients(self):\n",
    "        return self.attn_gradients\n",
    "    \n",
    "    def save_attention_map(self, attention_map):\n",
    "        self.attention_map = attention_map\n",
    "        \n",
    "    def get_attention_map(self):\n",
    "        return self.attention_map\n",
    "    \n",
    "    def forward(self, x, register_hook=False):\n",
    "        B, N, C = x.shape\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]   # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "                \n",
    "        if register_hook:\n",
    "            self.save_attention_map(attn)\n",
    "            attn.register_hook(self.save_attn_gradients)        \n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,\n",
    "                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm, use_grad_checkpointing=False):\n",
    "        super().__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(\n",
    "            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)\n",
    "        # NOTE: drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)\n",
    "\n",
    "        if use_grad_checkpointing:\n",
    "            self.attn = checkpoint_wrapper(self.attn)\n",
    "            self.mlp = checkpoint_wrapper(self.mlp)\n",
    "\n",
    "    def forward(self, x, register_hook=False):\n",
    "        x = x + self.drop_path(self.attn(self.norm1(x), register_hook=register_hook))\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "    \n",
    "class VisionTransformer(nn.Module):\n",
    "    \"\"\" Vision Transformer\n",
    "    A PyTorch impl of : `An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale`  -\n",
    "        https://arxiv.org/abs/2010.11929\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dim=768, depth=12,\n",
    "                 num_heads=12, mlp_ratio=4., qkv_bias=True, qk_scale=None, representation_size=None,\n",
    "                 drop_rate=0., attn_drop_rate=0., drop_path_rate=0., norm_layer=None, \n",
    "                 use_grad_checkpointing=False, ckpt_layer=0):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_size (int, tuple): input image size\n",
    "            patch_size (int, tuple): patch size\n",
    "            in_chans (int): number of input channels\n",
    "            num_classes (int): number of classes for classification head\n",
    "            embed_dim (int): embedding dimension\n",
    "            depth (int): depth of transformer\n",
    "            num_heads (int): number of attention heads\n",
    "            mlp_ratio (int): ratio of mlp hidden dim to embedding dim\n",
    "            qkv_bias (bool): enable bias for qkv if True\n",
    "            qk_scale (float): override default qk scale of head_dim ** -0.5 if set\n",
    "            representation_size (Optional[int]): enable and set representation layer (pre-logits) to this value if set\n",
    "            drop_rate (float): dropout rate\n",
    "            attn_drop_rate (float): attention dropout rate\n",
    "            drop_path_rate (float): stochastic depth rate\n",
    "            norm_layer: (nn.Module): normalization layer\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n",
    "\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # stochastic depth decay rule\n",
    "        self.blocks = nn.ModuleList([\n",
    "            Block(\n",
    "                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer,\n",
    "                use_grad_checkpointing=(use_grad_checkpointing and i>=depth-ckpt_layer)\n",
    "            )\n",
    "            for i in range(depth)])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        trunc_normal_(self.pos_embed, std=.02)\n",
    "        trunc_normal_(self.cls_token, std=.02)\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        return {'pos_embed', 'cls_token'}\n",
    "\n",
    "    def forward(self, x, register_blk=-1):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)\n",
    "\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # stole cls_tokens impl from Phil Wang, thanks\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "  \n",
    "        x = x + self.pos_embed[:,:x.size(1),:]\n",
    "        x = self.pos_drop(x)\n",
    "\n",
    "        for i,blk in enumerate(self.blocks):\n",
    "            x = blk(x, register_blk==i)\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    @torch.jit.ignore()\n",
    "    def load_pretrained(self, checkpoint_path, prefix=''):\n",
    "        _load_weights(self, checkpoint_path, prefix)\n",
    "        \n",
    "\n",
    "@torch.no_grad()\n",
    "def _load_weights(model: VisionTransformer, checkpoint_path: str, prefix: str = ''):\n",
    "    \"\"\" Load weights from .npz checkpoints for official Google Brain Flax implementation\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    def _n2p(w, t=True):\n",
    "        if w.ndim == 4 and w.shape[0] == w.shape[1] == w.shape[2] == 1:\n",
    "            w = w.flatten()\n",
    "        if t:\n",
    "            if w.ndim == 4:\n",
    "                w = w.transpose([3, 2, 0, 1])\n",
    "            elif w.ndim == 3:\n",
    "                w = w.transpose([2, 0, 1])\n",
    "            elif w.ndim == 2:\n",
    "                w = w.transpose([1, 0])\n",
    "        return torch.from_numpy(w)\n",
    "\n",
    "    w = np.load(checkpoint_path)\n",
    "    if not prefix and 'opt/target/embedding/kernel' in w:\n",
    "        prefix = 'opt/target/'\n",
    "\n",
    "    if hasattr(model.patch_embed, 'backbone'):\n",
    "        # hybrid\n",
    "        backbone = model.patch_embed.backbone\n",
    "        stem_only = not hasattr(backbone, 'stem')\n",
    "        stem = backbone if stem_only else backbone.stem\n",
    "        stem.conv.weight.copy_(adapt_input_conv(stem.conv.weight.shape[1], _n2p(w[f'{prefix}conv_root/kernel'])))\n",
    "        stem.norm.weight.copy_(_n2p(w[f'{prefix}gn_root/scale']))\n",
    "        stem.norm.bias.copy_(_n2p(w[f'{prefix}gn_root/bias']))\n",
    "        if not stem_only:\n",
    "            for i, stage in enumerate(backbone.stages):\n",
    "                for j, block in enumerate(stage.blocks):\n",
    "                    bp = f'{prefix}block{i + 1}/unit{j + 1}/'\n",
    "                    for r in range(3):\n",
    "                        getattr(block, f'conv{r + 1}').weight.copy_(_n2p(w[f'{bp}conv{r + 1}/kernel']))\n",
    "                        getattr(block, f'norm{r + 1}').weight.copy_(_n2p(w[f'{bp}gn{r + 1}/scale']))\n",
    "                        getattr(block, f'norm{r + 1}').bias.copy_(_n2p(w[f'{bp}gn{r + 1}/bias']))\n",
    "                    if block.downsample is not None:\n",
    "                        block.downsample.conv.weight.copy_(_n2p(w[f'{bp}conv_proj/kernel']))\n",
    "                        block.downsample.norm.weight.copy_(_n2p(w[f'{bp}gn_proj/scale']))\n",
    "                        block.downsample.norm.bias.copy_(_n2p(w[f'{bp}gn_proj/bias']))\n",
    "        embed_conv_w = _n2p(w[f'{prefix}embedding/kernel'])\n",
    "    else:\n",
    "        embed_conv_w = adapt_input_conv(\n",
    "            model.patch_embed.proj.weight.shape[1], _n2p(w[f'{prefix}embedding/kernel']))\n",
    "    model.patch_embed.proj.weight.copy_(embed_conv_w)\n",
    "    model.patch_embed.proj.bias.copy_(_n2p(w[f'{prefix}embedding/bias']))\n",
    "    model.cls_token.copy_(_n2p(w[f'{prefix}cls'], t=False))\n",
    "    pos_embed_w = _n2p(w[f'{prefix}Transformer/posembed_input/pos_embedding'], t=False)\n",
    "    if pos_embed_w.shape != model.pos_embed.shape:\n",
    "        pos_embed_w = resize_pos_embed(  # resize pos embedding when different size from pretrained weights\n",
    "            pos_embed_w, model.pos_embed, getattr(model, 'num_tokens', 1), model.patch_embed.grid_size)\n",
    "    model.pos_embed.copy_(pos_embed_w)\n",
    "    model.norm.weight.copy_(_n2p(w[f'{prefix}Transformer/encoder_norm/scale']))\n",
    "    model.norm.bias.copy_(_n2p(w[f'{prefix}Transformer/encoder_norm/bias']))\n",
    "#     if isinstance(model.head, nn.Linear) and model.head.bias.shape[0] == w[f'{prefix}head/bias'].shape[-1]:\n",
    "#         model.head.weight.copy_(_n2p(w[f'{prefix}head/kernel']))\n",
    "#         model.head.bias.copy_(_n2p(w[f'{prefix}head/bias']))\n",
    "#     if isinstance(getattr(model.pre_logits, 'fc', None), nn.Linear) and f'{prefix}pre_logits/bias' in w:\n",
    "#         model.pre_logits.fc.weight.copy_(_n2p(w[f'{prefix}pre_logits/kernel']))\n",
    "#         model.pre_logits.fc.bias.copy_(_n2p(w[f'{prefix}pre_logits/bias']))\n",
    "    for i, block in enumerate(model.blocks.children()):\n",
    "        block_prefix = f'{prefix}Transformer/encoderblock_{i}/'\n",
    "        mha_prefix = block_prefix + 'MultiHeadDotProductAttention_1/'\n",
    "        block.norm1.weight.copy_(_n2p(w[f'{block_prefix}LayerNorm_0/scale']))\n",
    "        block.norm1.bias.copy_(_n2p(w[f'{block_prefix}LayerNorm_0/bias']))\n",
    "        block.attn.qkv.weight.copy_(torch.cat([\n",
    "            _n2p(w[f'{mha_prefix}{n}/kernel'], t=False).flatten(1).T for n in ('query', 'key', 'value')]))\n",
    "        block.attn.qkv.bias.copy_(torch.cat([\n",
    "            _n2p(w[f'{mha_prefix}{n}/bias'], t=False).reshape(-1) for n in ('query', 'key', 'value')]))\n",
    "        block.attn.proj.weight.copy_(_n2p(w[f'{mha_prefix}out/kernel']).flatten(1))\n",
    "        block.attn.proj.bias.copy_(_n2p(w[f'{mha_prefix}out/bias']))\n",
    "        for r in range(2):\n",
    "            getattr(block.mlp, f'fc{r + 1}').weight.copy_(_n2p(w[f'{block_prefix}MlpBlock_3/Dense_{r}/kernel']))\n",
    "            getattr(block.mlp, f'fc{r + 1}').bias.copy_(_n2p(w[f'{block_prefix}MlpBlock_3/Dense_{r}/bias']))\n",
    "        block.norm2.weight.copy_(_n2p(w[f'{block_prefix}LayerNorm_2/scale']))\n",
    "        block.norm2.bias.copy_(_n2p(w[f'{block_prefix}LayerNorm_2/bias']))\n",
    "\n",
    "            \n",
    "def interpolate_pos_embed(pos_embed_checkpoint, visual_encoder):        \n",
    "    # interpolate position embedding\n",
    "    embedding_size = pos_embed_checkpoint.shape[-1]\n",
    "    num_patches = visual_encoder.patch_embed.num_patches\n",
    "    num_extra_tokens = visual_encoder.pos_embed.shape[-2] - num_patches\n",
    "    # height (== width) for the checkpoint position embedding\n",
    "    orig_size = int((pos_embed_checkpoint.shape[-2] - num_extra_tokens) ** 0.5)\n",
    "    # height (== width) for the new position embedding\n",
    "    new_size = int(num_patches ** 0.5)\n",
    "\n",
    "    if orig_size!=new_size:\n",
    "        # class_token and dist_token are kept unchanged\n",
    "        extra_tokens = pos_embed_checkpoint[:, :num_extra_tokens]\n",
    "        # only the position tokens are interpolated\n",
    "        pos_tokens = pos_embed_checkpoint[:, num_extra_tokens:]\n",
    "        pos_tokens = pos_tokens.reshape(-1, orig_size, orig_size, embedding_size).permute(0, 3, 1, 2)\n",
    "        pos_tokens = torch.nn.functional.interpolate(\n",
    "            pos_tokens, size=(new_size, new_size), mode='bicubic', align_corners=False)\n",
    "        pos_tokens = pos_tokens.permute(0, 2, 3, 1).flatten(1, 2)\n",
    "        new_pos_embed = torch.cat((extra_tokens, pos_tokens), dim=1)\n",
    "        print('reshape position embedding from %d to %d'%(orig_size ** 2,new_size ** 2))\n",
    "        \n",
    "        return new_pos_embed    \n",
    "    else:\n",
    "        return pos_embed_checkpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43fcfd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.get_logger(__name__)\n",
    "class BertEmbeddings(nn.Module):\n",
    "    \"\"\"Construct the embeddings from word and position embeddings.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n",
    "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
    "\n",
    "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
    "        # any TensorFlow checkpoint file\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n",
    "        self.register_buffer(\"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)))\n",
    "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
    "        \n",
    "        self.config = config\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids=None, position_ids=None, inputs_embeds=None, past_key_values_length=0\n",
    "    ):\n",
    "        if input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "        else:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "\n",
    "        seq_length = input_shape[1]\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = self.position_ids[:, past_key_values_length : seq_length + past_key_values_length]\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.word_embeddings(input_ids)\n",
    "\n",
    "        embeddings = inputs_embeds\n",
    "\n",
    "        if self.position_embedding_type == \"absolute\":\n",
    "            position_embeddings = self.position_embeddings(position_ids)\n",
    "            embeddings += position_embeddings\n",
    "        embeddings = self.LayerNorm(embeddings)\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class BertSelfAttention(nn.Module):\n",
    "    def __init__(self, config, is_cross_attention):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n",
    "            raise ValueError(\n",
    "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
    "                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads)\n",
    "            )\n",
    "        \n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        if is_cross_attention:\n",
    "            self.key = nn.Linear(config.encoder_width, self.all_head_size)\n",
    "            self.value = nn.Linear(config.encoder_width, self.all_head_size)\n",
    "        else:\n",
    "            self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "            self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "        self.position_embedding_type = getattr(config, \"position_embedding_type\", \"absolute\")\n",
    "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
    "            self.max_position_embeddings = config.max_position_embeddings\n",
    "            self.distance_embedding = nn.Embedding(2 * config.max_position_embeddings - 1, self.attention_head_size)\n",
    "        self.save_attention = False   \n",
    "            \n",
    "    def save_attn_gradients(self, attn_gradients):\n",
    "        self.attn_gradients = attn_gradients\n",
    "        \n",
    "    def get_attn_gradients(self):\n",
    "        return self.attn_gradients\n",
    "    \n",
    "    def save_attention_map(self, attention_map):\n",
    "        self.attention_map = attention_map\n",
    "        \n",
    "    def get_attention_map(self):\n",
    "        return self.attention_map\n",
    "    \n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "\n",
    "        # If this is instantiated as a cross-attention module, the keys\n",
    "        # and values come from an encoder; the attention mask needs to be\n",
    "        # such that the encoder's padding tokens are not attended to.\n",
    "        is_cross_attention = encoder_hidden_states is not None\n",
    "\n",
    "        if is_cross_attention:\n",
    "            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))\n",
    "            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))\n",
    "            attention_mask = encoder_attention_mask\n",
    "        elif past_key_value is not None:\n",
    "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)\n",
    "            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)\n",
    "        else:\n",
    "            key_layer = self.transpose_for_scores(self.key(hidden_states))\n",
    "            value_layer = self.transpose_for_scores(self.value(hidden_states))\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "\n",
    "        past_key_value = (key_layer, value_layer)\n",
    "\n",
    "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "\n",
    "        if self.position_embedding_type == \"relative_key\" or self.position_embedding_type == \"relative_key_query\":\n",
    "            seq_length = hidden_states.size()[1]\n",
    "            position_ids_l = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(-1, 1)\n",
    "            position_ids_r = torch.arange(seq_length, dtype=torch.long, device=hidden_states.device).view(1, -1)\n",
    "            distance = position_ids_l - position_ids_r\n",
    "            positional_embedding = self.distance_embedding(distance + self.max_position_embeddings - 1)\n",
    "            positional_embedding = positional_embedding.to(dtype=query_layer.dtype)  # fp16 compatibility\n",
    "\n",
    "            if self.position_embedding_type == \"relative_key\":\n",
    "                relative_position_scores = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
    "                attention_scores = attention_scores + relative_position_scores\n",
    "            elif self.position_embedding_type == \"relative_key_query\":\n",
    "                relative_position_scores_query = torch.einsum(\"bhld,lrd->bhlr\", query_layer, positional_embedding)\n",
    "                relative_position_scores_key = torch.einsum(\"bhrd,lrd->bhlr\", key_layer, positional_embedding)\n",
    "                attention_scores = attention_scores + relative_position_scores_query + relative_position_scores_key\n",
    "\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        if attention_mask is not None:\n",
    "            # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        # Normalize the attention scores to probabilities.\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "        \n",
    "        if is_cross_attention and self.save_attention:\n",
    "            self.save_attention_map(attention_probs)\n",
    "            attention_probs.register_hook(self.save_attn_gradients)         \n",
    "\n",
    "        # This is actually dropping out entire tokens to attend to, which might\n",
    "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
    "        attention_probs_dropped = self.dropout(attention_probs)\n",
    "\n",
    "        # Mask heads if we want to\n",
    "        if head_mask is not None:\n",
    "            attention_probs_dropped = attention_probs_dropped * head_mask\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs_dropped, value_layer)\n",
    "\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "\n",
    "        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n",
    "\n",
    "        outputs = outputs + (past_key_value,)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class BertSelfOutput(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class BertAttention(nn.Module):\n",
    "    def __init__(self, config, is_cross_attention=False):\n",
    "        super().__init__()\n",
    "        self.self = BertSelfAttention(config, is_cross_attention)\n",
    "        self.output = BertSelfOutput(config)\n",
    "        self.pruned_heads = set()\n",
    "\n",
    "    def prune_heads(self, heads):\n",
    "        if len(heads) == 0:\n",
    "            return\n",
    "        heads, index = find_pruneable_heads_and_indices(\n",
    "            heads, self.self.num_attention_heads, self.self.attention_head_size, self.pruned_heads\n",
    "        )\n",
    "\n",
    "        # Prune linear layers\n",
    "        self.self.query = prune_linear_layer(self.self.query, index)\n",
    "        self.self.key = prune_linear_layer(self.self.key, index)\n",
    "        self.self.value = prune_linear_layer(self.self.value, index)\n",
    "        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)\n",
    "\n",
    "        # Update hyper params and store pruned heads\n",
    "        self.self.num_attention_heads = self.self.num_attention_heads - len(heads)\n",
    "        self.self.all_head_size = self.self.attention_head_size * self.self.num_attention_heads\n",
    "        self.pruned_heads = self.pruned_heads.union(heads)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        self_outputs = self.self(\n",
    "            hidden_states,\n",
    "            attention_mask,\n",
    "            head_mask,\n",
    "            encoder_hidden_states,\n",
    "            encoder_attention_mask,\n",
    "            past_key_value,\n",
    "            output_attentions,\n",
    "        )\n",
    "        attention_output = self.output(self_outputs[0], hidden_states)\n",
    "        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n",
    "        return outputs\n",
    "\n",
    "\n",
    "class BertIntermediate(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        if isinstance(config.hidden_act, str):\n",
    "            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
    "        else:\n",
    "            self.intermediate_act_fn = config.hidden_act\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class BertOutput(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, hidden_states, input_tensor):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class BertLayer(nn.Module):\n",
    "    def __init__(self, config, layer_num):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.chunk_size_feed_forward = config.chunk_size_feed_forward\n",
    "        self.seq_len_dim = 1\n",
    "        self.attention = BertAttention(config)      \n",
    "        self.layer_num = layer_num          \n",
    "        if self.config.add_cross_attention:\n",
    "            self.crossattention = BertAttention(config, is_cross_attention=self.config.add_cross_attention)\n",
    "        self.intermediate = BertIntermediate(config)\n",
    "        self.output = BertOutput(config)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False,\n",
    "        mode=None,\n",
    "    ):\n",
    "        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n",
    "        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n",
    "        self_attention_outputs = self.attention(\n",
    "            hidden_states,\n",
    "            attention_mask,\n",
    "            head_mask,\n",
    "            output_attentions=output_attentions,\n",
    "            past_key_value=self_attn_past_key_value,\n",
    "        )\n",
    "        attention_output = self_attention_outputs[0]\n",
    "\n",
    "        outputs = self_attention_outputs[1:-1]\n",
    "        present_key_value = self_attention_outputs[-1]\n",
    "\n",
    "        if mode=='multimodal':\n",
    "            assert encoder_hidden_states is not None, \"encoder_hidden_states must be given for cross-attention layers\"\n",
    "\n",
    "            cross_attention_outputs = self.crossattention(\n",
    "                attention_output,\n",
    "                attention_mask,\n",
    "                head_mask,\n",
    "                encoder_hidden_states,\n",
    "                encoder_attention_mask,\n",
    "                output_attentions=output_attentions,\n",
    "            )\n",
    "            attention_output = cross_attention_outputs[0]\n",
    "            outputs = outputs + cross_attention_outputs[1:-1]  # add cross attentions if we output attention weights                               \n",
    "        layer_output = apply_chunking_to_forward(\n",
    "            self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n",
    "        )\n",
    "        outputs = (layer_output,) + outputs\n",
    "\n",
    "        outputs = outputs + (present_key_value,)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def feed_forward_chunk(self, attention_output):\n",
    "        intermediate_output = self.intermediate(attention_output)\n",
    "        layer_output = self.output(intermediate_output, attention_output)\n",
    "        return layer_output\n",
    "\n",
    "\n",
    "class BertEncoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.layer = nn.ModuleList([BertLayer(config,i) for i in range(config.num_hidden_layers)])\n",
    "        self.gradient_checkpointing = False\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_values=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=False,\n",
    "        output_hidden_states=False,\n",
    "        return_dict=True,\n",
    "        mode='multimodal',\n",
    "    ):\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attentions = () if output_attentions else None\n",
    "        all_cross_attentions = () if output_attentions and self.config.add_cross_attention else None\n",
    "\n",
    "        next_decoder_cache = () if use_cache else None\n",
    "               \n",
    "        for i in range(self.config.num_hidden_layers):\n",
    "            layer_module = self.layer[i]\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "            layer_head_mask = head_mask[i] if head_mask is not None else None\n",
    "            past_key_value = past_key_values[i] if past_key_values is not None else None\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "\n",
    "                if use_cache:\n",
    "                    logger.warn(\n",
    "                        \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n",
    "                    )\n",
    "                    use_cache = False\n",
    "\n",
    "                def create_custom_forward(module):\n",
    "                    def custom_forward(*inputs):\n",
    "                        return module(*inputs, past_key_value, output_attentions)\n",
    "\n",
    "                    return custom_forward\n",
    "\n",
    "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
    "                    create_custom_forward(layer_module),\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    layer_head_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                    mode=mode,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = layer_module(\n",
    "                    hidden_states,\n",
    "                    attention_mask,\n",
    "                    layer_head_mask,\n",
    "                    encoder_hidden_states,\n",
    "                    encoder_attention_mask,\n",
    "                    past_key_value,\n",
    "                    output_attentions,\n",
    "                    mode=mode,\n",
    "                )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "            if use_cache:\n",
    "                next_decoder_cache += (layer_outputs[-1],)\n",
    "            if output_attentions:\n",
    "                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n",
    "\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states = all_hidden_states + (hidden_states,)\n",
    "\n",
    "        if not return_dict:\n",
    "            return tuple(\n",
    "                v\n",
    "                for v in [\n",
    "                    hidden_states,\n",
    "                    next_decoder_cache,\n",
    "                    all_hidden_states,\n",
    "                    all_self_attentions,\n",
    "                    all_cross_attentions,\n",
    "                ]\n",
    "                if v is not None\n",
    "            )\n",
    "        return BaseModelOutputWithPastAndCrossAttentions(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=next_decoder_cache,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attentions,\n",
    "            cross_attentions=all_cross_attentions,\n",
    "        )\n",
    "\n",
    "\n",
    "class BertPooler(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        # We \"pool\" the model by simply taking the hidden state corresponding\n",
    "        # to the first token.\n",
    "        first_token_tensor = hidden_states[:, 0]\n",
    "        pooled_output = self.dense(first_token_tensor)\n",
    "        pooled_output = self.activation(pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "\n",
    "class BertPredictionHeadTransform(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        if isinstance(config.hidden_act, str):\n",
    "            self.transform_act_fn = ACT2FN[config.hidden_act]\n",
    "        else:\n",
    "            self.transform_act_fn = config.hidden_act\n",
    "        self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = self.transform_act_fn(hidden_states)\n",
    "        hidden_states = self.LayerNorm(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class BertLMPredictionHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.transform = BertPredictionHeadTransform(config)\n",
    "\n",
    "        # The output weights are the same as the input embeddings, but there is\n",
    "        # an output-only bias for each token.\n",
    "        self.decoder = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        self.bias = nn.Parameter(torch.zeros(config.vocab_size))\n",
    "\n",
    "        # Need a link between the two variables so that the bias is correctly resized with `resize_token_embeddings`\n",
    "        self.decoder.bias = self.bias\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.transform(hidden_states)\n",
    "        hidden_states = self.decoder(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "\n",
    "class BertOnlyMLMHead(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.predictions = BertLMPredictionHead(config)\n",
    "\n",
    "    def forward(self, sequence_output):\n",
    "        prediction_scores = self.predictions(sequence_output)\n",
    "        return prediction_scores\n",
    "\n",
    "\n",
    "class BertPreTrainedModel(PreTrainedModel):\n",
    "    \"\"\"\n",
    "    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n",
    "    models.\n",
    "    \"\"\"\n",
    "\n",
    "    config_class = BertConfig\n",
    "    base_model_prefix = \"bert\"\n",
    "    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        \"\"\" Initialize the weights \"\"\"\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
    "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
    "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "            module.bias.data.zero_()\n",
    "\n",
    "\n",
    "class BertModel(BertPreTrainedModel):\n",
    "    \"\"\"\n",
    "    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n",
    "    cross-attention is added between the self-attention layers, following the architecture described in `Attention is\n",
    "    all you need <https://arxiv.org/abs/1706.03762>`__ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\n",
    "    Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.\n",
    "    argument and :obj:`add_cross_attention` set to :obj:`True`; an :obj:`encoder_hidden_states` is then expected as an\n",
    "    input to the forward pass.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, add_pooling_layer=True):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "\n",
    "        self.embeddings = BertEmbeddings(config)\n",
    "        \n",
    "        self.encoder = BertEncoder(config)\n",
    "\n",
    "        self.pooler = BertPooler(config) if add_pooling_layer else None\n",
    "\n",
    "        self.init_weights()\n",
    " \n",
    "\n",
    "    def get_input_embeddings(self):\n",
    "        return self.embeddings.word_embeddings\n",
    "\n",
    "    def set_input_embeddings(self, value):\n",
    "        self.embeddings.word_embeddings = value\n",
    "\n",
    "    def _prune_heads(self, heads_to_prune):\n",
    "        \"\"\"\n",
    "        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n",
    "        class PreTrainedModel\n",
    "        \"\"\"\n",
    "        for layer, heads in heads_to_prune.items():\n",
    "            self.encoder.layer[layer].attention.prune_heads(heads)\n",
    "\n",
    "    \n",
    "    def get_extended_attention_mask(self, attention_mask: Tensor, input_shape: Tuple[int], device: device, is_decoder: bool) -> Tensor:\n",
    "        \"\"\"\n",
    "        Makes broadcastable attention and causal masks so that future and masked tokens are ignored.\n",
    "\n",
    "        Arguments:\n",
    "            attention_mask (:obj:`torch.Tensor`):\n",
    "                Mask with ones indicating tokens to attend to, zeros for tokens to ignore.\n",
    "            input_shape (:obj:`Tuple[int]`):\n",
    "                The shape of the input to the model.\n",
    "            device: (:obj:`torch.device`):\n",
    "                The device of the input to the model.\n",
    "\n",
    "        Returns:\n",
    "            :obj:`torch.Tensor` The extended attention mask, with a the same dtype as :obj:`attention_mask.dtype`.\n",
    "        \"\"\"\n",
    "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
    "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
    "        if attention_mask.dim() == 3:\n",
    "            extended_attention_mask = attention_mask[:, None, :, :]\n",
    "        elif attention_mask.dim() == 2:\n",
    "            # Provided a padding mask of dimensions [batch_size, seq_length]\n",
    "            # - if the model is a decoder, apply a causal mask in addition to the padding mask\n",
    "            # - if the model is an encoder, make the mask broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
    "            if is_decoder:\n",
    "                batch_size, seq_length = input_shape\n",
    "\n",
    "                seq_ids = torch.arange(seq_length, device=device)\n",
    "                causal_mask = seq_ids[None, None, :].repeat(batch_size, seq_length, 1) <= seq_ids[None, :, None]\n",
    "                # in case past_key_values are used we need to add a prefix ones mask to the causal mask\n",
    "                # causal and attention masks must have same type with pytorch version < 1.3\n",
    "                causal_mask = causal_mask.to(attention_mask.dtype)\n",
    "   \n",
    "                if causal_mask.shape[1] < attention_mask.shape[1]:\n",
    "                    prefix_seq_len = attention_mask.shape[1] - causal_mask.shape[1]\n",
    "                    causal_mask = torch.cat(\n",
    "                        [\n",
    "                            torch.ones((batch_size, seq_length, prefix_seq_len), device=device, dtype=causal_mask.dtype),\n",
    "                            causal_mask,\n",
    "                        ],\n",
    "                        axis=-1,\n",
    "                    )                     \n",
    "\n",
    "                extended_attention_mask = causal_mask[:, None, :, :] * attention_mask[:, None, None, :]\n",
    "            else:\n",
    "                extended_attention_mask = attention_mask[:, None, None, :]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"Wrong shape for input_ids (shape {}) or attention_mask (shape {})\".format(\n",
    "                    input_shape, attention_mask.shape\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
    "        # masked positions, this operation will create a tensor which is 0.0 for\n",
    "        # positions we want to attend and -10000.0 for masked positions.\n",
    "        # Since we are adding it to the raw scores before the softmax, this is\n",
    "        # effectively the same as removing these entirely.\n",
    "        extended_attention_mask = extended_attention_mask.to(dtype=self.dtype)  # fp16 compatibility\n",
    "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
    "        return extended_attention_mask\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        encoder_embeds=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_values=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        is_decoder=False,\n",
    "        mode='multimodal',\n",
    "    ):\n",
    "        r\"\"\"\n",
    "        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n",
    "            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n",
    "            the model is configured as a decoder.\n",
    "        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n",
    "            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n",
    "            - 1 for tokens that are **not masked**,\n",
    "            - 0 for tokens that are **masked**.\n",
    "        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
    "            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
    "            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n",
    "            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n",
    "            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n",
    "        use_cache (:obj:`bool`, `optional`):\n",
    "            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n",
    "            decoding (see :obj:`past_key_values`).\n",
    "        \"\"\"\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if is_decoder:\n",
    "            use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        else:\n",
    "            use_cache = False\n",
    "\n",
    "        if input_ids is not None and inputs_embeds is not None:\n",
    "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
    "        elif input_ids is not None:\n",
    "            input_shape = input_ids.size()\n",
    "            batch_size, seq_length = input_shape\n",
    "            device = input_ids.device\n",
    "        elif inputs_embeds is not None:\n",
    "            input_shape = inputs_embeds.size()[:-1]\n",
    "            batch_size, seq_length = input_shape\n",
    "            device = inputs_embeds.device\n",
    "        elif encoder_embeds is not None:    \n",
    "            input_shape = encoder_embeds.size()[:-1]\n",
    "            batch_size, seq_length = input_shape \n",
    "            device = encoder_embeds.device\n",
    "        else:\n",
    "            raise ValueError(\"You have to specify either input_ids or inputs_embeds or encoder_embeds\")\n",
    "\n",
    "        # past_key_values_length\n",
    "        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
    "\n",
    "        if attention_mask is None:\n",
    "            attention_mask = torch.ones(((batch_size, seq_length + past_key_values_length)), device=device)\n",
    "            \n",
    "        # We can provide a self-attention mask of dimensions [batch_size, from_seq_length, to_seq_length]\n",
    "        # ourselves in which case we just need to make it broadcastable to all heads.\n",
    "        extended_attention_mask: torch.Tensor = self.get_extended_attention_mask(attention_mask, input_shape, \n",
    "                                                                                 device, is_decoder)\n",
    "\n",
    "        # If a 2D or 3D attention mask is provided for the cross-attention\n",
    "        # we need to make broadcastable to [batch_size, num_heads, seq_length, seq_length]\n",
    "        if encoder_hidden_states is not None:\n",
    "            if type(encoder_hidden_states) == list:\n",
    "                encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states[0].size()\n",
    "            else:\n",
    "                encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
    "            encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
    "            \n",
    "            if type(encoder_attention_mask) == list:\n",
    "                encoder_extended_attention_mask = [self.invert_attention_mask(mask) for mask in encoder_attention_mask]\n",
    "            elif encoder_attention_mask is None:\n",
    "                encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
    "                encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
    "            else:    \n",
    "                encoder_extended_attention_mask = self.invert_attention_mask(encoder_attention_mask)\n",
    "        else:\n",
    "            encoder_extended_attention_mask = None\n",
    "\n",
    "        # Prepare head mask if needed\n",
    "        # 1.0 in head_mask indicate we keep the head\n",
    "        # attention_probs has shape bsz x n_heads x N x N\n",
    "        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n",
    "        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n",
    "        head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n",
    "        \n",
    "        if encoder_embeds is None:\n",
    "            embedding_output = self.embeddings(\n",
    "                input_ids=input_ids,\n",
    "                position_ids=position_ids,\n",
    "                inputs_embeds=inputs_embeds,\n",
    "                past_key_values_length=past_key_values_length,\n",
    "            )\n",
    "        else:\n",
    "            embedding_output = encoder_embeds\n",
    "            \n",
    "        encoder_outputs = self.encoder(\n",
    "            embedding_output,\n",
    "            attention_mask=extended_attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_extended_attention_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            mode=mode,\n",
    "        )\n",
    "        sequence_output = encoder_outputs[0]\n",
    "        pooled_output = self.pooler(sequence_output) if self.pooler is not None else None\n",
    "\n",
    "        if not return_dict:\n",
    "            return (sequence_output, pooled_output) + encoder_outputs[1:]\n",
    "\n",
    "        return BaseModelOutputWithPoolingAndCrossAttentions(\n",
    "            last_hidden_state=sequence_output,\n",
    "            pooler_output=pooled_output,\n",
    "            past_key_values=encoder_outputs.past_key_values,\n",
    "            hidden_states=encoder_outputs.hidden_states,\n",
    "            attentions=encoder_outputs.attentions,\n",
    "            cross_attentions=encoder_outputs.cross_attentions,\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "class BertLMHeadModel(BertPreTrainedModel):\n",
    "\n",
    "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
    "    _keys_to_ignore_on_load_missing = [r\"position_ids\", r\"predictions.decoder.bias\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        self.cls = BertOnlyMLMHead(config)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def get_output_embeddings(self):\n",
    "        return self.cls.predictions.decoder\n",
    "\n",
    "    def set_output_embeddings(self, new_embeddings):\n",
    "        self.cls.predictions.decoder = new_embeddings\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        labels=None,\n",
    "        past_key_values=None,\n",
    "        use_cache=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        return_logits=False,            \n",
    "        is_decoder=True,\n",
    "        reduction='mean',\n",
    "        mode='multimodal', \n",
    "    ):\n",
    "        r\"\"\"\n",
    "        encoder_hidden_states  (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n",
    "            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n",
    "            the model is configured as a decoder.\n",
    "        encoder_attention_mask (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n",
    "            the cross-attention if the model is configured as a decoder. Mask values selected in ``[0, 1]``:\n",
    "            - 1 for tokens that are **not masked**,\n",
    "            - 0 for tokens that are **masked**.\n",
    "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
    "            Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\n",
    "            ``[-100, 0, ..., config.vocab_size]`` (see ``input_ids`` docstring) Tokens with indices set to ``-100`` are\n",
    "            ignored (masked), the loss is only computed for the tokens with labels n ``[0, ..., config.vocab_size]``\n",
    "        past_key_values (:obj:`tuple(tuple(torch.FloatTensor))` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
    "            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n",
    "            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`\n",
    "            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`\n",
    "            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.\n",
    "        use_cache (:obj:`bool`, `optional`):\n",
    "            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up\n",
    "            decoding (see :obj:`past_key_values`).\n",
    "        Returns:\n",
    "        Example::\n",
    "            >>> from transformers import BertTokenizer, BertLMHeadModel, BertConfig\n",
    "            >>> import torch\n",
    "            >>> tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "            >>> config = BertConfig.from_pretrained(\"bert-base-cased\")\n",
    "            >>> model = BertLMHeadModel.from_pretrained('bert-base-cased', config=config)\n",
    "            >>> inputs = tokenizer(\"Hello, my dog is cute\", return_tensors=\"pt\")\n",
    "            >>> outputs = model(**inputs)\n",
    "            >>> prediction_logits = outputs.logits\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        if labels is not None:\n",
    "            use_cache = False\n",
    "\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            encoder_hidden_states=encoder_hidden_states,\n",
    "            encoder_attention_mask=encoder_attention_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            is_decoder=is_decoder,\n",
    "            mode=mode,\n",
    "        )\n",
    "        \n",
    "        sequence_output = outputs[0]\n",
    "        prediction_scores = self.cls(sequence_output)\n",
    "        \n",
    "        if return_logits:\n",
    "            return prediction_scores[:, :-1, :].contiguous()  \n",
    "\n",
    "        lm_loss = None\n",
    "        if labels is not None:\n",
    "            # we are doing next-token prediction; shift prediction scores and input ids by one\n",
    "            shifted_prediction_scores = prediction_scores[:, :-1, :].contiguous()\n",
    "            labels = labels[:, 1:].contiguous()\n",
    "            loss_fct = CrossEntropyLoss(reduction=reduction, label_smoothing=0.1) \n",
    "            lm_loss = loss_fct(shifted_prediction_scores.view(-1, self.config.vocab_size), labels.view(-1))\n",
    "            if reduction=='none':\n",
    "                lm_loss = lm_loss.view(prediction_scores.size(0),-1).sum(1)               \n",
    "\n",
    "        if not return_dict:\n",
    "            output = (prediction_scores,) + outputs[2:]\n",
    "            return ((lm_loss,) + output) if lm_loss is not None else output\n",
    "\n",
    "        return CausalLMOutputWithCrossAttentions(\n",
    "            loss=lm_loss,\n",
    "            logits=prediction_scores,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "            cross_attentions=outputs.cross_attentions,\n",
    "        )\n",
    "\n",
    "    def prepare_inputs_for_generation(self, input_ids, past=None, attention_mask=None, **model_kwargs):\n",
    "        input_shape = input_ids.shape\n",
    "        # if model is used as a decoder in encoder-decoder model, the decoder attention mask is created on the fly\n",
    "        if attention_mask is None:\n",
    "            attention_mask = input_ids.new_ones(input_shape)\n",
    "\n",
    "        # cut decoder_input_ids if past is used\n",
    "        if past is not None:\n",
    "            input_ids = input_ids[:, -1:]\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids, \n",
    "            \"attention_mask\": attention_mask, \n",
    "            \"past_key_values\": past,\n",
    "            \"encoder_hidden_states\": model_kwargs.get(\"encoder_hidden_states\", None),\n",
    "            \"encoder_attention_mask\": model_kwargs.get(\"encoder_attention_mask\", None),\n",
    "            \"is_decoder\": True,\n",
    "        }\n",
    "\n",
    "    def _reorder_cache(self, past, beam_idx):\n",
    "        reordered_past = ()\n",
    "        for layer_past in past:\n",
    "            reordered_past += (tuple(past_state.index_select(0, beam_idx) for past_state in layer_past),)\n",
    "        return reordered_past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eadca1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# from models.vit import VisionTransformer, interpolate_pos_embed\n",
    "# from models.med import BertConfig, BertModel, BertLMHeadModel\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "from timm.models.hub import download_cached_file\n",
    "\n",
    "class BLIP_Base(nn.Module):\n",
    "    def __init__(self,                 \n",
    "                 med_config = 'configs/med_config.json',  \n",
    "                 image_size = 224,\n",
    "                 vit = 'base',\n",
    "                 vit_grad_ckpt = False,\n",
    "                 vit_ckpt_layer = 0,                 \n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            med_config (str): path for the mixture of encoder-decoder model's configuration file\n",
    "            image_size (int): input image size\n",
    "            vit (str): model size of vision transformer\n",
    "        \"\"\"               \n",
    "        super().__init__()\n",
    "        \n",
    "        self.visual_encoder, vision_width = create_vit(vit,image_size, vit_grad_ckpt, vit_ckpt_layer)\n",
    "        self.tokenizer = init_tokenizer()   \n",
    "        med_config = BertConfig.from_json_file(med_config)\n",
    "        med_config.encoder_width = vision_width\n",
    "        self.text_encoder = BertModel(config=med_config, add_pooling_layer=False)  \n",
    "\n",
    "        \n",
    "    def forward(self, image, caption, mode):\n",
    "        \n",
    "        assert mode in ['image', 'text', 'multimodal'], \"mode parameter must be image, text, or multimodal\"\n",
    "        text = self.tokenizer(caption, return_tensors=\"pt\").to(image.device) \n",
    "        \n",
    "        if mode=='image':    \n",
    "            # return image features\n",
    "            image_embeds = self.visual_encoder(image)             \n",
    "            return image_embeds\n",
    "        \n",
    "        elif mode=='text':\n",
    "            # return text features\n",
    "            text_output = self.text_encoder(text.input_ids, attention_mask = text.attention_mask,                      \n",
    "                                            return_dict = True, mode = 'text')  \n",
    "            return text_output.last_hidden_state\n",
    "        \n",
    "        elif mode=='multimodal':\n",
    "            # return multimodel features\n",
    "            image_embeds = self.visual_encoder(image)    \n",
    "            image_atts = torch.ones(image_embeds.size()[:-1],dtype=torch.long).to(image.device)      \n",
    "            \n",
    "            text.input_ids[:,0] = self.tokenizer.enc_token_id\n",
    "            output = self.text_encoder(text.input_ids,\n",
    "                                       attention_mask = text.attention_mask,\n",
    "                                       encoder_hidden_states = image_embeds,\n",
    "                                       encoder_attention_mask = image_atts,      \n",
    "                                       return_dict = True,\n",
    "                                      )              \n",
    "            return output.last_hidden_state\n",
    "        \n",
    "        \n",
    "        \n",
    "class BLIP_Decoder(nn.Module):\n",
    "    def __init__(self,                 \n",
    "                 med_config = 'configs/med_config.json',  \n",
    "                 image_size = 384,\n",
    "                 vit = 'base',\n",
    "                 vit_grad_ckpt = False,\n",
    "                 vit_ckpt_layer = 0,\n",
    "                 prompt = 'a picture of ',\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            med_config (str): path for the mixture of encoder-decoder model's configuration file\n",
    "            image_size (int): input image size\n",
    "            vit (str): model size of vision transformer\n",
    "        \"\"\"            \n",
    "        super().__init__()\n",
    "        \n",
    "        self.visual_encoder, vision_width = create_vit(vit,image_size, vit_grad_ckpt, vit_ckpt_layer)\n",
    "        self.tokenizer = init_tokenizer()   \n",
    "        med_config = BertConfig.from_json_file(med_config)\n",
    "        med_config.encoder_width = vision_width\n",
    "        self.text_decoder = BertLMHeadModel(config=med_config)    \n",
    "        \n",
    "        self.prompt = prompt\n",
    "        self.prompt_length = len(self.tokenizer(self.prompt).input_ids)-1\n",
    "\n",
    "        \n",
    "    def forward(self, image, caption):\n",
    "        \n",
    "        image_embeds = self.visual_encoder(image) \n",
    "        image_atts = torch.ones(image_embeds.size()[:-1],dtype=torch.long).to(image.device)\n",
    "        \n",
    "        text = self.tokenizer(caption, padding='longest', truncation=True, max_length=40, return_tensors=\"pt\").to(image.device) \n",
    "        \n",
    "        text.input_ids[:,0] = self.tokenizer.bos_token_id\n",
    "        \n",
    "        decoder_targets = text.input_ids.masked_fill(text.input_ids == self.tokenizer.pad_token_id, -100)         \n",
    "        decoder_targets[:,:self.prompt_length] = -100\n",
    "     \n",
    "        decoder_output = self.text_decoder(text.input_ids, \n",
    "                                           attention_mask = text.attention_mask, \n",
    "                                           encoder_hidden_states = image_embeds,\n",
    "                                           encoder_attention_mask = image_atts,                  \n",
    "                                           labels = decoder_targets,\n",
    "                                           return_dict = True,   \n",
    "                                          )   \n",
    "        loss_lm = decoder_output.loss\n",
    "        \n",
    "        return loss_lm\n",
    "        \n",
    "    def generate(self, image, sample=False, num_beams=3, max_length=30, min_length=10, top_p=0.9, repetition_penalty=1.0):\n",
    "        image_embeds = self.visual_encoder(image)\n",
    "\n",
    "        if not sample:\n",
    "            image_embeds = image_embeds.repeat_interleave(num_beams,dim=0)\n",
    "            \n",
    "        image_atts = torch.ones(image_embeds.size()[:-1],dtype=torch.long).to(image.device)\n",
    "        model_kwargs = {\"encoder_hidden_states\": image_embeds, \"encoder_attention_mask\":image_atts}\n",
    "        \n",
    "        prompt = [self.prompt] * image.size(0)\n",
    "        input_ids = self.tokenizer(prompt, return_tensors=\"pt\").input_ids.to(image.device) \n",
    "        input_ids[:,0] = self.tokenizer.bos_token_id\n",
    "        input_ids = input_ids[:, :-1] \n",
    "\n",
    "        if sample:\n",
    "            #nucleus sampling\n",
    "            outputs = self.text_decoder.generate(input_ids=input_ids,\n",
    "                                                  max_length=max_length,\n",
    "                                                  min_length=min_length,\n",
    "                                                  do_sample=True,\n",
    "                                                  top_p=top_p,\n",
    "                                                  num_return_sequences=1,\n",
    "                                                  eos_token_id=self.tokenizer.sep_token_id,\n",
    "                                                  pad_token_id=self.tokenizer.pad_token_id, \n",
    "                                                  repetition_penalty=1.1,                                            \n",
    "                                                  **model_kwargs)\n",
    "        else:\n",
    "            #beam search\n",
    "            outputs = self.text_decoder.generate(input_ids=input_ids,\n",
    "                                                  max_length=max_length,\n",
    "                                                  min_length=min_length,\n",
    "                                                  num_beams=num_beams,\n",
    "                                                  eos_token_id=self.tokenizer.sep_token_id,\n",
    "                                                  pad_token_id=self.tokenizer.pad_token_id,     \n",
    "                                                  repetition_penalty=repetition_penalty,\n",
    "                                                  **model_kwargs)            \n",
    "            \n",
    "        captions = []    \n",
    "        for output in outputs:\n",
    "            caption = self.tokenizer.decode(output, skip_special_tokens=True)    \n",
    "            captions.append(caption[len(self.prompt):])\n",
    "        return captions\n",
    "    \n",
    "\n",
    "def blip_decoder(pretrained='',**kwargs):\n",
    "    model = BLIP_Decoder(**kwargs)\n",
    "    if pretrained:\n",
    "        model,msg = load_checkpoint(model,pretrained)\n",
    "        assert(len(msg.missing_keys)==0)\n",
    "    return model    \n",
    "    \n",
    "def blip_feature_extractor(pretrained='',**kwargs):\n",
    "    model = BLIP_Base(**kwargs)\n",
    "    if pretrained:\n",
    "        model,msg = load_checkpoint(model,pretrained)\n",
    "        assert(len(msg.missing_keys)==0)\n",
    "    return model        \n",
    "\n",
    "def init_tokenizer():\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    tokenizer.add_special_tokens({'bos_token':'[DEC]'})\n",
    "    tokenizer.add_special_tokens({'additional_special_tokens':['[ENC]']})       \n",
    "    tokenizer.enc_token_id = tokenizer.additional_special_tokens_ids[0]  \n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def create_vit(vit, image_size, use_grad_checkpointing=False, ckpt_layer=0, drop_path_rate=0):\n",
    "        \n",
    "    assert vit in ['base', 'large'], \"vit parameter must be base or large\"\n",
    "    if vit=='base':\n",
    "        vision_width = 768\n",
    "        visual_encoder = VisionTransformer(img_size=image_size, patch_size=16, embed_dim=vision_width, depth=12, \n",
    "                                           num_heads=12, use_grad_checkpointing=use_grad_checkpointing, ckpt_layer=ckpt_layer,\n",
    "                                           drop_path_rate=0 or drop_path_rate\n",
    "                                          )   \n",
    "    elif vit=='large':\n",
    "        vision_width = 1024\n",
    "        visual_encoder = VisionTransformer(img_size=image_size, patch_size=16, embed_dim=vision_width, depth=24, \n",
    "                                           num_heads=16, use_grad_checkpointing=use_grad_checkpointing, ckpt_layer=ckpt_layer,\n",
    "                                           drop_path_rate=0.1 or drop_path_rate\n",
    "                                          )   \n",
    "    return visual_encoder, vision_width\n",
    "\n",
    "def is_url(url_or_filename):\n",
    "    parsed = urlparse(url_or_filename)\n",
    "    return parsed.scheme in (\"http\", \"https\")\n",
    "\n",
    "def load_checkpoint(model,url_or_filename):\n",
    "    if is_url(url_or_filename):\n",
    "        cached_file = download_cached_file(url_or_filename, check_hash=False, progress=True)\n",
    "        checkpoint = torch.load(cached_file, map_location='cpu') \n",
    "    elif os.path.isfile(url_or_filename):        \n",
    "        checkpoint = torch.load(url_or_filename, map_location='cpu') \n",
    "    else:\n",
    "        raise RuntimeError('checkpoint url or path is invalid')\n",
    "        \n",
    "    state_dict = checkpoint['model']\n",
    "    \n",
    "    state_dict['visual_encoder.pos_embed'] = interpolate_pos_embed(state_dict['visual_encoder.pos_embed'],model.visual_encoder) \n",
    "    if 'visual_encoder_m.pos_embed' in model.state_dict().keys():\n",
    "        state_dict['visual_encoder_m.pos_embed'] = interpolate_pos_embed(state_dict['visual_encoder_m.pos_embed'],\n",
    "                                                                         model.visual_encoder_m)    \n",
    "    for key in model.state_dict().keys():\n",
    "        if key in state_dict.keys():\n",
    "            if state_dict[key].shape!=model.state_dict()[key].shape:\n",
    "                del state_dict[key]\n",
    "    \n",
    "    msg = model.load_state_dict(state_dict,strict=False)\n",
    "    print('load checkpoint from %s'%url_or_filename)  \n",
    "    return model,msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1850ce11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def cosine_lr_schedule(optimizer, epoch, max_epoch, init_lr, min_lr):\n",
    "    \"\"\"Decay the learning rate\"\"\"\n",
    "    lr = (init_lr - min_lr) * 0.5 * (1. + math.cos(math.pi * epoch / max_epoch)) + min_lr\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "        \n",
    "def warmup_lr_schedule(optimizer, step, max_step, init_lr, max_lr):\n",
    "    \"\"\"Warmup the learning rate\"\"\"\n",
    "    lr = min(max_lr, init_lr + (max_lr - init_lr) * step / max_step)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr    \n",
    "\n",
    "def step_lr_schedule(optimizer, epoch, init_lr, min_lr, decay_rate):        \n",
    "    \"\"\"Decay the learning rate\"\"\"\n",
    "    lr = max(min_lr, init_lr * (decay_rate**epoch))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr    \n",
    "        \n",
    "import numpy as np\n",
    "import io\n",
    "import os\n",
    "import time\n",
    "from collections import defaultdict, deque\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "\n",
    "class SmoothedValue(object):\n",
    "    \"\"\"Track a series of values and provide access to smoothed values over a\n",
    "    window or the global series average.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, window_size=20, fmt=None):\n",
    "        if fmt is None:\n",
    "            fmt = \"{median:.4f} ({global_avg:.4f})\"\n",
    "        self.deque = deque(maxlen=window_size)\n",
    "        self.total = 0.0\n",
    "        self.count = 0\n",
    "        self.fmt = fmt\n",
    "\n",
    "    def update(self, value, n=1):\n",
    "        self.deque.append(value)\n",
    "        self.count += n\n",
    "        self.total += value * n\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        \"\"\"\n",
    "        Warning: does not synchronize the deque!\n",
    "        \"\"\"\n",
    "        if not is_dist_avail_and_initialized():\n",
    "            return\n",
    "        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\n",
    "        dist.barrier()\n",
    "        dist.all_reduce(t)\n",
    "        t = t.tolist()\n",
    "        self.count = int(t[0])\n",
    "        self.total = t[1]\n",
    "\n",
    "    @property\n",
    "    def median(self):\n",
    "        d = torch.tensor(list(self.deque))\n",
    "        return d.median().item()\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "        d = torch.tensor(list(self.deque), dtype=torch.float32)\n",
    "        return d.mean().item()\n",
    "\n",
    "    @property\n",
    "    def global_avg(self):\n",
    "        return self.total / self.count\n",
    "\n",
    "    @property\n",
    "    def max(self):\n",
    "        return max(self.deque)\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        return self.deque[-1]\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.fmt.format(\n",
    "            median=self.median,\n",
    "            avg=self.avg,\n",
    "            global_avg=self.global_avg,\n",
    "            max=self.max,\n",
    "            value=self.value)\n",
    "\n",
    "\n",
    "class MetricLogger(object):\n",
    "    def __init__(self, delimiter=\"\\t\"):\n",
    "        self.meters = defaultdict(SmoothedValue)\n",
    "        self.delimiter = delimiter\n",
    "\n",
    "    def update(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                v = v.item()\n",
    "            assert isinstance(v, (float, int))\n",
    "            self.meters[k].update(v)\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        if attr in self.meters:\n",
    "            return self.meters[attr]\n",
    "        if attr in self.__dict__:\n",
    "            return self.__dict__[attr]\n",
    "        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
    "            type(self).__name__, attr))\n",
    "\n",
    "    def __str__(self):\n",
    "        loss_str = []\n",
    "        for name, meter in self.meters.items():\n",
    "            loss_str.append(\n",
    "                \"{}: {}\".format(name, str(meter))\n",
    "            )\n",
    "        return self.delimiter.join(loss_str)\n",
    "\n",
    "    def global_avg(self):\n",
    "        loss_str = []\n",
    "        for name, meter in self.meters.items():\n",
    "            loss_str.append(\n",
    "                \"{}: {:.4f}\".format(name, meter.global_avg)\n",
    "            )\n",
    "        return self.delimiter.join(loss_str)    \n",
    "    \n",
    "    def synchronize_between_processes(self):\n",
    "        for meter in self.meters.values():\n",
    "            meter.synchronize_between_processes()\n",
    "\n",
    "    def add_meter(self, name, meter):\n",
    "        self.meters[name] = meter\n",
    "\n",
    "    def log_every(self, iterable, print_freq, header=None):\n",
    "        i = 0\n",
    "        if not header:\n",
    "            header = ''\n",
    "        start_time = time.time()\n",
    "        end = time.time()\n",
    "        iter_time = SmoothedValue(fmt='{avg:.4f}')\n",
    "        data_time = SmoothedValue(fmt='{avg:.4f}')\n",
    "        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'\n",
    "        log_msg = [\n",
    "            header,\n",
    "            '[{0' + space_fmt + '}/{1}]',\n",
    "            'eta: {eta}',\n",
    "            '{meters}',\n",
    "            'time: {time}',\n",
    "            'data: {data}'\n",
    "        ]\n",
    "        if torch.cuda.is_available():\n",
    "            log_msg.append('max mem: {memory:.0f}')\n",
    "        log_msg = self.delimiter.join(log_msg)\n",
    "        MB = 1024.0 * 1024.0\n",
    "        for obj in iterable:\n",
    "            data_time.update(time.time() - end)\n",
    "            yield obj\n",
    "            iter_time.update(time.time() - end)\n",
    "            if i % print_freq == 0 or i == len(iterable) - 1:\n",
    "                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n",
    "                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
    "                if torch.cuda.is_available():\n",
    "                    print(log_msg.format(\n",
    "                        i, len(iterable), eta=eta_string,\n",
    "                        meters=str(self),\n",
    "                        time=str(iter_time), data=str(data_time),\n",
    "                        memory=torch.cuda.max_memory_allocated() / MB))\n",
    "                else:\n",
    "                    print(log_msg.format(\n",
    "                        i, len(iterable), eta=eta_string,\n",
    "                        meters=str(self),\n",
    "                        time=str(iter_time), data=str(data_time)))\n",
    "            i += 1\n",
    "            end = time.time()\n",
    "        total_time = time.time() - start_time\n",
    "        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "        print('{} Total time: {} ({:.4f} s / it)'.format(\n",
    "            header, total_time_str, total_time / len(iterable)))\n",
    "        \n",
    "\n",
    "class AttrDict(dict):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(AttrDict, self).__init__(*args, **kwargs)\n",
    "        self.__dict__ = self\n",
    "\n",
    "\n",
    "def compute_acc(logits, label, reduction='mean'):\n",
    "    ret = (torch.argmax(logits, dim=1) == label).float()\n",
    "    if reduction == 'none':\n",
    "        return ret.detach()\n",
    "    elif reduction == 'mean':\n",
    "        return ret.mean().item()\n",
    "\n",
    "def compute_n_params(model, return_str=True):\n",
    "    tot = 0\n",
    "    for p in model.parameters():\n",
    "        w = 1\n",
    "        for x in p.shape:\n",
    "            w *= x\n",
    "        tot += w\n",
    "    if return_str:\n",
    "        if tot >= 1e6:\n",
    "            return '{:.1f}M'.format(tot / 1e6)\n",
    "        else:\n",
    "            return '{:.1f}K'.format(tot / 1e3)\n",
    "    else:\n",
    "        return tot\n",
    "\n",
    "def setup_for_distributed(is_master):\n",
    "    \"\"\"\n",
    "    This function disables printing when not in master process\n",
    "    \"\"\"\n",
    "    import builtins as __builtin__\n",
    "    builtin_print = __builtin__.print\n",
    "\n",
    "    def print(*args, **kwargs):\n",
    "        force = kwargs.pop('force', False)\n",
    "        if is_master or force:\n",
    "            builtin_print(*args, **kwargs)\n",
    "\n",
    "    __builtin__.print = print\n",
    "\n",
    "\n",
    "def is_dist_avail_and_initialized():\n",
    "    if not dist.is_available():\n",
    "        return False\n",
    "    if not dist.is_initialized():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_world_size():\n",
    "    if not is_dist_avail_and_initialized():\n",
    "        return 1\n",
    "    return dist.get_world_size()\n",
    "\n",
    "\n",
    "def get_rank():\n",
    "    if not is_dist_avail_and_initialized():\n",
    "        return 0\n",
    "    return dist.get_rank()\n",
    "\n",
    "\n",
    "def is_main_process():\n",
    "    return get_rank() == 0\n",
    "\n",
    "\n",
    "def save_on_master(*args, **kwargs):\n",
    "    if is_main_process():\n",
    "        torch.save(*args, **kwargs)\n",
    "\n",
    "\n",
    "def init_distributed_mode(args):\n",
    "    if 'RANK' in os.environ and 'WORLD_SIZE' in os.environ:\n",
    "        args.rank = int(os.environ[\"RANK\"])\n",
    "        args.world_size = int(os.environ['WORLD_SIZE'])\n",
    "        args.gpu = int(os.environ['LOCAL_RANK'])\n",
    "    elif 'SLURM_PROCID' in os.environ:\n",
    "        args.rank = int(os.environ['SLURM_PROCID'])\n",
    "        args.gpu = args.rank % torch.cuda.device_count()\n",
    "    else:\n",
    "        print('Not using distributed mode')\n",
    "        args.distributed = False\n",
    "        return\n",
    "\n",
    "    args.distributed = True\n",
    "\n",
    "    torch.cuda.set_device(args.gpu)\n",
    "    args.dist_backend = 'nccl'\n",
    "    print('| distributed init (rank {}, word {}): {}'.format(\n",
    "        args.rank, args.world_size, args.dist_url), flush=True)\n",
    "    torch.distributed.init_process_group(backend=args.dist_backend, init_method=args.dist_url,\n",
    "                                         world_size=args.world_size, rank=args.rank)\n",
    "    torch.distributed.barrier()\n",
    "    setup_for_distributed(args.rank == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df98fb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "## aug functions\n",
    "def identity_func(img):\n",
    "    return img\n",
    "\n",
    "\n",
    "def autocontrast_func(img, cutoff=0):\n",
    "    '''\n",
    "        same output as PIL.ImageOps.autocontrast\n",
    "    '''\n",
    "    n_bins = 256\n",
    "\n",
    "    def tune_channel(ch):\n",
    "        n = ch.size\n",
    "        cut = cutoff * n // 100\n",
    "        if cut == 0:\n",
    "            high, low = ch.max(), ch.min()\n",
    "        else:\n",
    "            hist = cv2.calcHist([ch], [0], None, [n_bins], [0, n_bins])\n",
    "            low = np.argwhere(np.cumsum(hist) > cut)\n",
    "            low = 0 if low.shape[0] == 0 else low[0]\n",
    "            high = np.argwhere(np.cumsum(hist[::-1]) > cut)\n",
    "            high = n_bins - 1 if high.shape[0] == 0 else n_bins - 1 - high[0]\n",
    "        if high <= low:\n",
    "            table = np.arange(n_bins)\n",
    "        else:\n",
    "            scale = (n_bins - 1) / (high - low)\n",
    "            offset = -low * scale\n",
    "            table = np.arange(n_bins) * scale + offset\n",
    "            table[table < 0] = 0\n",
    "            table[table > n_bins - 1] = n_bins - 1\n",
    "        table = table.clip(0, 255).astype(np.uint8)\n",
    "        return table[ch]\n",
    "\n",
    "    channels = [tune_channel(ch) for ch in cv2.split(img)]\n",
    "    out = cv2.merge(channels)\n",
    "    return out\n",
    "\n",
    "\n",
    "def equalize_func(img):\n",
    "    '''\n",
    "        same output as PIL.ImageOps.equalize\n",
    "        PIL's implementation is different from cv2.equalize\n",
    "    '''\n",
    "    n_bins = 256\n",
    "\n",
    "    def tune_channel(ch):\n",
    "        hist = cv2.calcHist([ch], [0], None, [n_bins], [0, n_bins])\n",
    "        non_zero_hist = hist[hist != 0].reshape(-1)\n",
    "        step = np.sum(non_zero_hist[:-1]) // (n_bins - 1)\n",
    "        if step == 0: return ch\n",
    "        n = np.empty_like(hist)\n",
    "        n[0] = step // 2\n",
    "        n[1:] = hist[:-1]\n",
    "        table = (np.cumsum(n) // step).clip(0, 255).astype(np.uint8)\n",
    "        return table[ch]\n",
    "\n",
    "    channels = [tune_channel(ch) for ch in cv2.split(img)]\n",
    "    out = cv2.merge(channels)\n",
    "    return out\n",
    "\n",
    "\n",
    "def rotate_func(img, degree, fill=(0, 0, 0)):\n",
    "    '''\n",
    "    like PIL, rotate by degree, not radians\n",
    "    '''\n",
    "    H, W = img.shape[0], img.shape[1]\n",
    "    center = W / 2, H / 2\n",
    "    M = cv2.getRotationMatrix2D(center, degree, 1)\n",
    "    out = cv2.warpAffine(img, M, (W, H), borderValue=fill)\n",
    "    return out\n",
    "\n",
    "\n",
    "def solarize_func(img, thresh=128):\n",
    "    '''\n",
    "        same output as PIL.ImageOps.posterize\n",
    "    '''\n",
    "    table = np.array([el if el < thresh else 255 - el for el in range(256)])\n",
    "    table = table.clip(0, 255).astype(np.uint8)\n",
    "    out = table[img]\n",
    "    return out\n",
    "\n",
    "\n",
    "def color_func(img, factor):\n",
    "    '''\n",
    "        same output as PIL.ImageEnhance.Color\n",
    "    '''\n",
    "    ## implementation according to PIL definition, quite slow\n",
    "    #  degenerate = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)[:, :, np.newaxis]\n",
    "    #  out = blend(degenerate, img, factor)\n",
    "    #  M = (\n",
    "    #      np.eye(3) * factor\n",
    "    #      + np.float32([0.114, 0.587, 0.299]).reshape(3, 1) * (1. - factor)\n",
    "    #  )[np.newaxis, np.newaxis, :]\n",
    "    M = (\n",
    "            np.float32([\n",
    "                [0.886, -0.114, -0.114],\n",
    "                [-0.587, 0.413, -0.587],\n",
    "                [-0.299, -0.299, 0.701]]) * factor\n",
    "            + np.float32([[0.114], [0.587], [0.299]])\n",
    "    )\n",
    "    out = np.matmul(img, M).clip(0, 255).astype(np.uint8)\n",
    "    return out\n",
    "\n",
    "\n",
    "def contrast_func(img, factor):\n",
    "    \"\"\"\n",
    "        same output as PIL.ImageEnhance.Contrast\n",
    "    \"\"\"\n",
    "    mean = np.sum(np.mean(img, axis=(0, 1)) * np.array([0.114, 0.587, 0.299]))\n",
    "    table = np.array([(\n",
    "        el - mean) * factor + mean\n",
    "        for el in range(256)\n",
    "    ]).clip(0, 255).astype(np.uint8)\n",
    "    out = table[img]\n",
    "    return out\n",
    "\n",
    "\n",
    "def brightness_func(img, factor):\n",
    "    '''\n",
    "        same output as PIL.ImageEnhance.Contrast\n",
    "    '''\n",
    "    table = (np.arange(256, dtype=np.float32) * factor).clip(0, 255).astype(np.uint8)\n",
    "    out = table[img]\n",
    "    return out\n",
    "\n",
    "\n",
    "def sharpness_func(img, factor):\n",
    "    '''\n",
    "    The differences the this result and PIL are all on the 4 boundaries, the center\n",
    "    areas are same\n",
    "    '''\n",
    "    kernel = np.ones((3, 3), dtype=np.float32)\n",
    "    kernel[1][1] = 5\n",
    "    kernel /= 13\n",
    "    degenerate = cv2.filter2D(img, -1, kernel)\n",
    "    if factor == 0.0:\n",
    "        out = degenerate\n",
    "    elif factor == 1.0:\n",
    "        out = img\n",
    "    else:\n",
    "        out = img.astype(np.float32)\n",
    "        degenerate = degenerate.astype(np.float32)[1:-1, 1:-1, :]\n",
    "        out[1:-1, 1:-1, :] = degenerate + factor * (out[1:-1, 1:-1, :] - degenerate)\n",
    "        out = out.astype(np.uint8)\n",
    "    return out\n",
    "\n",
    "\n",
    "def shear_x_func(img, factor, fill=(0, 0, 0)):\n",
    "    H, W = img.shape[0], img.shape[1]\n",
    "    M = np.float32([[1, factor, 0], [0, 1, 0]])\n",
    "    out = cv2.warpAffine(img, M, (W, H), borderValue=fill, flags=cv2.INTER_LINEAR).astype(np.uint8)\n",
    "    return out\n",
    "\n",
    "\n",
    "def translate_x_func(img, offset, fill=(0, 0, 0)):\n",
    "    '''\n",
    "        same output as PIL.Image.transform\n",
    "    '''\n",
    "    H, W = img.shape[0], img.shape[1]\n",
    "    M = np.float32([[1, 0, -offset], [0, 1, 0]])\n",
    "    out = cv2.warpAffine(img, M, (W, H), borderValue=fill, flags=cv2.INTER_LINEAR).astype(np.uint8)\n",
    "    return out\n",
    "\n",
    "\n",
    "def translate_y_func(img, offset, fill=(0, 0, 0)):\n",
    "    '''\n",
    "        same output as PIL.Image.transform\n",
    "    '''\n",
    "    H, W = img.shape[0], img.shape[1]\n",
    "    M = np.float32([[1, 0, 0], [0, 1, -offset]])\n",
    "    out = cv2.warpAffine(img, M, (W, H), borderValue=fill, flags=cv2.INTER_LINEAR).astype(np.uint8)\n",
    "    return out\n",
    "\n",
    "\n",
    "def posterize_func(img, bits):\n",
    "    '''\n",
    "        same output as PIL.ImageOps.posterize\n",
    "    '''\n",
    "    out = np.bitwise_and(img, np.uint8(255 << (8 - bits)))\n",
    "    return out\n",
    "\n",
    "\n",
    "def shear_y_func(img, factor, fill=(0, 0, 0)):\n",
    "    H, W = img.shape[0], img.shape[1]\n",
    "    M = np.float32([[1, 0, 0], [factor, 1, 0]])\n",
    "    out = cv2.warpAffine(img, M, (W, H), borderValue=fill, flags=cv2.INTER_LINEAR).astype(np.uint8)\n",
    "    return out\n",
    "\n",
    "\n",
    "def cutout_func(img, pad_size, replace=(0, 0, 0)):\n",
    "    replace = np.array(replace, dtype=np.uint8)\n",
    "    H, W = img.shape[0], img.shape[1]\n",
    "    rh, rw = np.random.random(2)\n",
    "    pad_size = pad_size // 2\n",
    "    ch, cw = int(rh * H), int(rw * W)\n",
    "    x1, x2 = max(ch - pad_size, 0), min(ch + pad_size, H)\n",
    "    y1, y2 = max(cw - pad_size, 0), min(cw + pad_size, W)\n",
    "    out = img.copy()\n",
    "    out[x1:x2, y1:y2, :] = replace\n",
    "    return out\n",
    "\n",
    "\n",
    "### level to args\n",
    "def enhance_level_to_args(MAX_LEVEL):\n",
    "    def level_to_args(level):\n",
    "        return ((level / MAX_LEVEL) * 1.8 + 0.1,)\n",
    "    return level_to_args\n",
    "\n",
    "\n",
    "def shear_level_to_args(MAX_LEVEL, replace_value):\n",
    "    def level_to_args(level):\n",
    "        level = (level / MAX_LEVEL) * 0.3\n",
    "        if np.random.random() > 0.5: level = -level\n",
    "        return (level, replace_value)\n",
    "\n",
    "    return level_to_args\n",
    "\n",
    "\n",
    "def translate_level_to_args(translate_const, MAX_LEVEL, replace_value):\n",
    "    def level_to_args(level):\n",
    "        level = (level / MAX_LEVEL) * float(translate_const)\n",
    "        if np.random.random() > 0.5: level = -level\n",
    "        return (level, replace_value)\n",
    "\n",
    "    return level_to_args\n",
    "\n",
    "\n",
    "def cutout_level_to_args(cutout_const, MAX_LEVEL, replace_value):\n",
    "    def level_to_args(level):\n",
    "        level = int((level / MAX_LEVEL) * cutout_const)\n",
    "        return (level, replace_value)\n",
    "\n",
    "    return level_to_args\n",
    "\n",
    "\n",
    "def solarize_level_to_args(MAX_LEVEL):\n",
    "    def level_to_args(level):\n",
    "        level = int((level / MAX_LEVEL) * 256)\n",
    "        return (level, )\n",
    "    return level_to_args\n",
    "\n",
    "\n",
    "def none_level_to_args(level):\n",
    "    return ()\n",
    "\n",
    "\n",
    "def posterize_level_to_args(MAX_LEVEL):\n",
    "    def level_to_args(level):\n",
    "        level = int((level / MAX_LEVEL) * 4)\n",
    "        return (level, )\n",
    "    return level_to_args\n",
    "\n",
    "\n",
    "def rotate_level_to_args(MAX_LEVEL, replace_value):\n",
    "    def level_to_args(level):\n",
    "        level = (level / MAX_LEVEL) * 30\n",
    "        if np.random.random() < 0.5:\n",
    "            level = -level\n",
    "        return (level, replace_value)\n",
    "\n",
    "    return level_to_args\n",
    "\n",
    "\n",
    "func_dict = {\n",
    "    'Identity': identity_func,\n",
    "    'AutoContrast': autocontrast_func,\n",
    "    'Equalize': equalize_func,\n",
    "    'Rotate': rotate_func,\n",
    "    'Solarize': solarize_func,\n",
    "    'Color': color_func,\n",
    "    'Contrast': contrast_func,\n",
    "    'Brightness': brightness_func,\n",
    "    'Sharpness': sharpness_func,\n",
    "    'ShearX': shear_x_func,\n",
    "    'TranslateX': translate_x_func,\n",
    "    'TranslateY': translate_y_func,\n",
    "    'Posterize': posterize_func,\n",
    "    'ShearY': shear_y_func,\n",
    "}\n",
    "\n",
    "translate_const = 10\n",
    "MAX_LEVEL = 10\n",
    "replace_value = (128, 128, 128)\n",
    "arg_dict = {\n",
    "    'Identity': none_level_to_args,\n",
    "    'AutoContrast': none_level_to_args,\n",
    "    'Equalize': none_level_to_args,\n",
    "    'Rotate': rotate_level_to_args(MAX_LEVEL, replace_value),\n",
    "    'Solarize': solarize_level_to_args(MAX_LEVEL),\n",
    "    'Color': enhance_level_to_args(MAX_LEVEL),\n",
    "    'Contrast': enhance_level_to_args(MAX_LEVEL),\n",
    "    'Brightness': enhance_level_to_args(MAX_LEVEL),\n",
    "    'Sharpness': enhance_level_to_args(MAX_LEVEL),\n",
    "    'ShearX': shear_level_to_args(MAX_LEVEL, replace_value),\n",
    "    'TranslateX': translate_level_to_args(\n",
    "        translate_const, MAX_LEVEL, replace_value\n",
    "    ),\n",
    "    'TranslateY': translate_level_to_args(\n",
    "        translate_const, MAX_LEVEL, replace_value\n",
    "    ),\n",
    "    'Posterize': posterize_level_to_args(MAX_LEVEL),\n",
    "    'ShearY': shear_level_to_args(MAX_LEVEL, replace_value),\n",
    "}\n",
    "\n",
    "\n",
    "class RandomAugment(object):\n",
    "\n",
    "    def __init__(self, N=2, M=10, isPIL=False, augs=[]):\n",
    "        self.N = N\n",
    "        self.M = M\n",
    "        self.isPIL = isPIL\n",
    "        if augs:\n",
    "            self.augs = augs       \n",
    "        else:\n",
    "            self.augs = list(arg_dict.keys())\n",
    "\n",
    "    def get_random_ops(self):\n",
    "        sampled_ops = np.random.choice(self.augs, self.N)\n",
    "        return [(op, 0.5, self.M) for op in sampled_ops]\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if self.isPIL:\n",
    "            img = np.array(img)            \n",
    "        ops = self.get_random_ops()\n",
    "        for name, prob, level in ops:\n",
    "            if np.random.random() > prob:\n",
    "                continue\n",
    "            args = arg_dict[name](level)\n",
    "            img = func_dict[name](img, *args) \n",
    "        return img\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     a = RandomAugment()\n",
    "#     img = np.random.randn(32, 32, 3)\n",
    "#     a(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "032bb595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "\n",
    "# from transform.randaugment import RandomAugment\n",
    "\n",
    "def create_dataset(dataset, config, min_scale=0.5):\n",
    "    \n",
    "    normalize = transforms.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "\n",
    "    transform_train = transforms.Compose([                        \n",
    "            transforms.RandomResizedCrop(config['image_size'],scale=(min_scale, 1.0),interpolation=InterpolationMode.BICUBIC),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            RandomAugment(2,5,isPIL=True,augs=['Identity','AutoContrast','Brightness','Sharpness','Equalize',\n",
    "                                              'ShearX', 'ShearY', 'TranslateX', 'TranslateY', 'Rotate']),     \n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])        \n",
    "    transform_test = transforms.Compose([\n",
    "        transforms.Resize((config['image_size'],config['image_size']),interpolation=InterpolationMode.BICUBIC),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "        ])  \n",
    "         \n",
    "    if dataset=='caption_flickr':\n",
    "        train_dataset = flickr30k_train_caption(transform_train, config['image_root'], config['ann_root'], prompt=config['prompt'])\n",
    "        val_dataset = flickr30k_eval_caption(transform_test, config['image_root'], config['ann_root'], 'val')\n",
    "        test_dataset = flickr30k_eval_caption(transform_test, config['image_root'], config['ann_root'], 'test')   \n",
    "        return train_dataset, val_dataset, test_dataset\n",
    "    \n",
    "    \n",
    "def create_sampler(datasets, shuffles, num_tasks, global_rank):\n",
    "    samplers = []\n",
    "    for dataset,shuffle in zip(datasets,shuffles):\n",
    "        sampler = torch.utils.data.DistributedSampler(dataset, num_replicas=num_tasks, rank=global_rank, shuffle=shuffle)\n",
    "        samplers.append(sampler)\n",
    "    return samplers     \n",
    "\n",
    "\n",
    "def create_loader(datasets, samplers, batch_size, num_workers, is_trains, collate_fns):\n",
    "    loaders = []\n",
    "    for dataset,sampler,bs,n_worker,is_train,collate_fn in zip(datasets,samplers,batch_size,num_workers,is_trains,collate_fns):\n",
    "        if is_train:\n",
    "            shuffle = (sampler is None)\n",
    "            drop_last = True\n",
    "        else:\n",
    "            shuffle = False\n",
    "            drop_last = False\n",
    "        loader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=bs,\n",
    "            num_workers=n_worker,\n",
    "            pin_memory=True,\n",
    "            sampler=sampler,\n",
    "            shuffle=shuffle,\n",
    "            collate_fn=collate_fn,\n",
    "            drop_last=drop_last,\n",
    "        )              \n",
    "        loaders.append(loader)\n",
    "    return loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a62546b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.coco import COCO\n",
    "from pycocoevalcap.eval import COCOEvalCap\n",
    "from torchvision.datasets.utils import download_url\n",
    "\n",
    "def coco_caption_eval(coco_gt_root, results_file, split):\n",
    "    urls = {'val':'https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_val_gt.json',\n",
    "            'test':'https://storage.googleapis.com/sfr-vision-language-research/datasets/coco_karpathy_test_gt.json'}\n",
    "    filenames = {'val':'coco_karpathy_val_gt.json','test':'coco_karpathy_test_gt.json'}    \n",
    "    \n",
    "    download_url(urls[split],coco_gt_root)\n",
    "    annotation_file = os.path.join(coco_gt_root,filenames[split])\n",
    "    \n",
    "    # create coco object and coco_result object\n",
    "    coco = COCO(annotation_file)\n",
    "    coco_result = coco.loadRes(results_file)\n",
    "\n",
    "    # create coco_eval object by taking coco and coco_result\n",
    "    coco_eval = COCOEvalCap(coco, coco_result)\n",
    "\n",
    "    # evaluate on a subset of images by setting\n",
    "    # coco_eval.params['image_id'] = coco_result.getImgIds()\n",
    "    # please remove this line when evaluating the full validation set\n",
    "    # coco_eval.params['image_id'] = coco_result.getImgIds()\n",
    "\n",
    "    # evaluate results\n",
    "    # SPICE will take a few minutes the first time, but speeds up due to caching\n",
    "    coco_eval.evaluate()\n",
    "\n",
    "    # print output evaluation scores\n",
    "    for metric, score in coco_eval.eval.items():\n",
    "        print(f'{metric}: {score:.3f}')\n",
    "    \n",
    "    return coco_eval\n",
    "\n",
    "def save_result(result, result_dir, filename, remove_duplicate=''):\n",
    "    result_file = os.path.join(result_dir, '%s_rank%d.json'%(filename, get_rank()))\n",
    "    final_result_file = os.path.join(result_dir, '%s.json'%filename)\n",
    "    \n",
    "    json.dump(result,open(result_file,'w'))\n",
    "\n",
    "    dist.barrier()\n",
    "\n",
    "    if is_main_process():   \n",
    "        # combine results from all processes\n",
    "        result = []\n",
    "\n",
    "        for rank in range(get_world_size()):\n",
    "            result_file = os.path.join(result_dir, '%s_rank%d.json'%(filename,rank))\n",
    "            res = json.load(open(result_file,'r'))\n",
    "            result += res\n",
    "\n",
    "        if remove_duplicate:\n",
    "            result_new = []\n",
    "            id_list = []    \n",
    "            for res in result:\n",
    "                if res[remove_duplicate] not in id_list:\n",
    "                    id_list.append(res[remove_duplicate])\n",
    "                    result_new.append(res)\n",
    "            result = result_new             \n",
    "                \n",
    "        json.dump(result,open(final_result_file,'w'))            \n",
    "        print('result file saved to %s'%final_result_file)\n",
    "\n",
    "    return final_result_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cea4ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, optimizer, epoch, device):\n",
    "    # train\n",
    "    model.train()  \n",
    "    \n",
    "    metric_logger = MetricLogger(delimiter=\"  \")\n",
    "    metric_logger.add_meter('lr', SmoothedValue(window_size=1, fmt='{value:.6f}'))\n",
    "    metric_logger.add_meter('loss', SmoothedValue(window_size=1, fmt='{value:.4f}'))\n",
    "    header = 'Train Caption Epoch: [{}]'.format(epoch)\n",
    "    print_freq = 50\n",
    "\n",
    "    for i, (image, caption, _) in enumerate(metric_logger.log_every(data_loader, print_freq, header)):\n",
    "        image = image.to(device)       \n",
    "        \n",
    "        loss = model(image, caption)      \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()    \n",
    "        \n",
    "        metric_logger.update(loss=loss.item())\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "\n",
    "    # gather the stats from all processes\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print(\"Averaged stats:\", metric_logger.global_avg())     \n",
    "    return {k: \"{:.3f}\".format(meter.global_avg) for k, meter in metric_logger.meters.items()}  \n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data_loader, device, config):\n",
    "    # evaluate\n",
    "    model.eval() \n",
    "    \n",
    "    metric_logger = MetricLogger(delimiter=\"  \")\n",
    "    header = 'Caption generation:'\n",
    "    print_freq = 10\n",
    "\n",
    "    result = []\n",
    "    for image, image_id in metric_logger.log_every(data_loader, print_freq, header): \n",
    "        \n",
    "        image = image.to(device)       \n",
    "        \n",
    "        captions = model.generate(image, sample=False, num_beams=config['num_beams'], max_length=config['max_length'], \n",
    "                                  min_length=config['min_length'])\n",
    "        \n",
    "        for caption, img_id in zip(captions, image_id):\n",
    "            result.append({\"image_id\": img_id.item(), \"caption\": caption})\n",
    "  \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bea702bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args, config):\n",
    "    init_distributed_mode(args)    \n",
    "    \n",
    "    device = torch.device(args.device)\n",
    "\n",
    "    # fix the seed for reproducibility\n",
    "    seed = args.seed + get_rank()\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    #### Dataset #### \n",
    "    print(\"Creating captioning dataset\")\n",
    "    train_dataset, val_dataset, test_dataset = create_dataset('caption_flickr', config)  \n",
    "\n",
    "    if args.distributed:\n",
    "        num_tasks = get_world_size()\n",
    "        global_rank = get_rank()            \n",
    "        samplers = create_sampler([train_dataset,val_dataset,test_dataset], [True,False,False], num_tasks, global_rank)         \n",
    "    else:\n",
    "        samplers = [None, None, None]\n",
    "    \n",
    "    train_loader, val_loader, test_loader = create_loader([train_dataset, val_dataset, test_dataset],samplers,\n",
    "                                                          batch_size=[config['batch_size']]*3,num_workers=[0,0,0],\n",
    "                                                          is_trains=[True, False, False], collate_fns=[None,None,None])         \n",
    "\n",
    "    #### Model #### \n",
    "    print(\"Creating model\")\n",
    "    model = blip_decoder(pretrained=config['pretrained'], image_size=config['image_size'], vit=config['vit'], \n",
    "                           vit_grad_ckpt=config['vit_grad_ckpt'], vit_ckpt_layer=config['vit_ckpt_layer'], \n",
    "                           prompt=config['prompt'])\n",
    "\n",
    "    model = model.to(device)   \n",
    "    \n",
    "    model_without_ddp = model\n",
    "    if args.distributed:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(model, device_ids=[args.gpu])\n",
    "        model_without_ddp = model.module    \n",
    "    \n",
    "    optimizer = torch.optim.AdamW(params=model.parameters(), lr=config['init_lr'], weight_decay=config['weight_decay'])\n",
    "            \n",
    "    best = 0\n",
    "    best_epoch = 0\n",
    "\n",
    "    print(\"Start training\")\n",
    "    start_time = time.time()    \n",
    "    for epoch in range(0, config['max_epoch']):\n",
    "        if not args.evaluate:        \n",
    "            if args.distributed:\n",
    "                train_loader.sampler.set_epoch(epoch)\n",
    "                \n",
    "            cosine_lr_schedule(optimizer, epoch, config['max_epoch'], config['init_lr'], config['min_lr'])\n",
    "                \n",
    "            train_stats = train(model, train_loader, optimizer, epoch, device) \n",
    "        \n",
    "        val_result = evaluate(model_without_ddp, val_loader, device, config)  \n",
    "        val_result_file = save_result(val_result, args.result_dir, 'val_epoch%d'%epoch, remove_duplicate='image_id')        \n",
    "  \n",
    "        test_result = evaluate(model_without_ddp, test_loader, device, config)  \n",
    "        test_result_file = save_result(test_result, args.result_dir, 'test_epoch%d'%epoch, remove_duplicate='image_id')  \n",
    "\n",
    "        if is_main_process():   \n",
    "            coco_val = coco_caption_eval(config['coco_gt_root'],val_result_file,'val')\n",
    "            coco_test = coco_caption_eval(config['coco_gt_root'],test_result_file,'test')\n",
    "            \n",
    "            if args.evaluate:            \n",
    "                log_stats = {**{f'val_{k}': v for k, v in coco_val.eval.items()},\n",
    "                             **{f'test_{k}': v for k, v in coco_test.eval.items()},                       \n",
    "                            }\n",
    "                with open(os.path.join(args.output_dir, \"evaluate.txt\"),\"a\") as f:\n",
    "                    f.write(json.dumps(log_stats) + \"\\n\")                   \n",
    "            else:             \n",
    "                save_obj = {\n",
    "                    'model': model_without_ddp.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'config': config,\n",
    "                    'epoch': epoch,\n",
    "                }\n",
    "\n",
    "                if coco_val.eval['CIDEr'] + coco_val.eval['Bleu_4'] > best:\n",
    "                    best = coco_val.eval['CIDEr'] + coco_val.eval['Bleu_4']\n",
    "                    best_epoch = epoch                \n",
    "                    torch.save(save_obj, os.path.join(args.output_dir, 'checkpoint_best.pth')) \n",
    "                    \n",
    "                log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n",
    "                             **{f'val_{k}': v for k, v in coco_val.eval.items()},\n",
    "                             **{f'test_{k}': v for k, v in coco_test.eval.items()},                       \n",
    "                             'epoch': epoch,\n",
    "                             'best_epoch': best_epoch,\n",
    "                            }\n",
    "                with open(os.path.join(args.output_dir, \"log.txt\"),\"a\") as f:\n",
    "                    f.write(json.dumps(log_stats) + \"\\n\")     \n",
    "                    \n",
    "        if args.evaluate: \n",
    "            break\n",
    "        dist.barrier()     \n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "    print('Training time {}'.format(total_time_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "630115c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not using distributed mode\n",
      "Creating captioning dataset\n",
      "Creating model\n",
      "load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_caption_capfilt_large.pth\n",
      "Start training\n",
      "Train Caption Epoch: [0]  [   0/4531]  eta: 1 day, 12:31:16  lr: 0.000010  loss: 3.6523  time: 29.0171  data: 0.2050\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     17\u001b[39m Path(args.result_dir).mkdir(parents=\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     19\u001b[39m yaml.dump(config, \u001b[38;5;28mopen\u001b[39m(os.path.join(args.output_dir, \u001b[33m'\u001b[39m\u001b[33mconfig.yaml\u001b[39m\u001b[33m'\u001b[39m), \u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m))    \n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m(args, config)\u001b[39m\n\u001b[32m     51\u001b[39m         train_loader.sampler.set_epoch(epoch)\n\u001b[32m     53\u001b[39m     cosine_lr_schedule(optimizer, epoch, config[\u001b[33m'\u001b[39m\u001b[33mmax_epoch\u001b[39m\u001b[33m'\u001b[39m], config[\u001b[33m'\u001b[39m\u001b[33minit_lr\u001b[39m\u001b[33m'\u001b[39m], config[\u001b[33m'\u001b[39m\u001b[33mmin_lr\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m     train_stats = \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \n\u001b[32m     57\u001b[39m val_result = evaluate(model_without_ddp, val_loader, device, config)  \n\u001b[32m     58\u001b[39m val_result_file = save_result(val_result, args.result_dir, \u001b[33m'\u001b[39m\u001b[33mval_epoch\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m'\u001b[39m%epoch, remove_duplicate=\u001b[33m'\u001b[39m\u001b[33mimage_id\u001b[39m\u001b[33m'\u001b[39m)        \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, data_loader, optimizer, epoch, device)\u001b[39m\n\u001b[32m     16\u001b[39m optimizer.zero_grad()\n\u001b[32m     17\u001b[39m loss.backward()\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m    \n\u001b[32m     20\u001b[39m metric_logger.update(loss=loss.item())\n\u001b[32m     21\u001b[39m metric_logger.update(lr=optimizer.param_groups[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/torch/optim/optimizer.py:504\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    499\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    500\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    501\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    502\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m504\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    507\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/torch/optim/optimizer.py:79\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     77\u001b[39m     torch.set_grad_enabled(\u001b[38;5;28mself\u001b[39m.defaults[\u001b[33m\"\u001b[39m\u001b[33mdifferentiable\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     78\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     81\u001b[39m     torch._dynamo.graph_break()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/torch/optim/adam.py:247\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    235\u001b[39m     beta1, beta2 = group[\u001b[33m\"\u001b[39m\u001b[33mbetas\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    237\u001b[39m     has_complex = \u001b[38;5;28mself\u001b[39m._init_group(\n\u001b[32m    238\u001b[39m         group,\n\u001b[32m    239\u001b[39m         params_with_grad,\n\u001b[32m   (...)\u001b[39m\u001b[32m    244\u001b[39m         state_steps,\n\u001b[32m    245\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m247\u001b[39m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    248\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    249\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mamsgrad\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43meps\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmaximize\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mforeach\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcapturable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdifferentiable\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfused\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgrad_scale\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfound_inf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdecoupled_weight_decay\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/torch/optim/optimizer.py:147\u001b[39m, in \u001b[36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(*args, **kwargs)\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/torch/optim/adam.py:940\u001b[39m, in \u001b[36madam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, decoupled_weight_decay, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[39m\n\u001b[32m    937\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    938\u001b[39m     func = _single_tensor_adam\n\u001b[32m--> \u001b[39m\u001b[32m940\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    941\u001b[39m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    942\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m=\u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    949\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    950\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    951\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    952\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    953\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m=\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    954\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    955\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    956\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    957\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    958\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    959\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecoupled_weight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    960\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/torch/optim/adam.py:409\u001b[39m, in \u001b[36m_single_tensor_adam\u001b[39m\u001b[34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable, decoupled_weight_decay)\u001b[39m\n\u001b[32m    406\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m weight_decay != \u001b[32m0\u001b[39m:\n\u001b[32m    407\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m decoupled_weight_decay:\n\u001b[32m    408\u001b[39m         \u001b[38;5;66;03m# Perform stepweight decay\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m         \u001b[43mparam\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    410\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    411\u001b[39m         \u001b[38;5;66;03m# Nested if is necessary to bypass jitscript rules\u001b[39;00m\n\u001b[32m    412\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m differentiable \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(weight_decay, Tensor):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "namespace = dict()\n",
    "namespace['output_dir'] = 'output/caption_flickr'\n",
    "namespace['device'] = 'mps'\n",
    "namespace['seed'] = 42\n",
    "namespace['world_size'] = 1\n",
    "namespace['dist_url'] = 'env://'\n",
    "namespace['distributed'] = True\n",
    "namespace['evaluate'] = False\n",
    "namespace['config'] = 'config.yaml'\n",
    "args = argparse.Namespace(**namespace)\n",
    "\n",
    "config = yaml.load(open(args.config, 'r'), Loader=yaml.Loader)\n",
    "\n",
    "args.result_dir = os.path.join(args.output_dir, 'result')\n",
    "\n",
    "Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
    "Path(args.result_dir).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "yaml.dump(config, open(os.path.join(args.output_dir, 'config.yaml'), 'w'))    \n",
    "\n",
    "main(args, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb021bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'config.yaml'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "namespace.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd5052f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
