{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0153510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import os\n",
    "from glob import glob\n",
    "from transformers import get_cosine_schedule_with_warmup, ViTForImageClassification\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from lightning.pytorch.loggers import MLFlowLogger\n",
    "import ray\n",
    "from ray.train import RunConfig, ScalingConfig\n",
    "from ray.train.torch import TorchTrainer\n",
    "import ray.train\n",
    "import ray.train.lightning\n",
    "\n",
    "\n",
    "import mlflow\n",
    "from torchmetrics.classification import BinaryAccuracy, BinaryF1Score, BinaryAUROC\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8374d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ray.init()\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:8080\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c369be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dee6339",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, csv_file, img_dir, transform=None):\n",
    "        self.annotations = csv_file\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.annotations.iloc[idx, 0])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = torch.tensor(int(self.annotations.iloc[idx, 1]))\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fd1ae4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloaders(csv_file, img_dir, img_size=(224, 224), batch_size=32, n_fold=0):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(img_size),\n",
    "        transforms.ToTensor(), \n",
    "        transforms.RandomHorizontalFlip()\n",
    "    ])\n",
    "\n",
    "    dataset = CustomImageDataset(csv_file=csv_file, img_dir=img_dir, transform=transform)\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=2024)\n",
    "    for i, (train_index, val_index) in enumerate(skf.split(np.zeros(len(csv_file)), csv_file.iloc[:, 1].values)):\n",
    "        if i == n_fold:\n",
    "            break\n",
    "            \n",
    "    train_dataset = Subset(dataset, train_index)\n",
    "    dataset = CustomImageDataset(csv_file=csv_file, img_dir=img_dir, \n",
    "                               transform=transforms.Compose([\n",
    "                                   transforms.Resize(img_size), \n",
    "                                   transforms.ToTensor()\n",
    "                               ]))\n",
    "    val_dataset = Subset(dataset, val_index)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cf7d37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_func(config):\n",
    "    # Set up MLFlow\n",
    "    mlflow_logger = MLFlowLogger(\n",
    "        experiment_name=\"vit-ai-detection\",\n",
    "        tracking_uri=mlflow.get_tracking_uri(),\n",
    "        run_name=f\"fold-{config['n_fold']}\"\n",
    "    )\n",
    "\n",
    "    # Preparing data\n",
    "    train_loader, val_loader = create_dataloaders(\n",
    "        csv_file=config[\"labels\"],\n",
    "        img_dir=config[\"img_dir\"],\n",
    "        img_size=config[\"img_size\"],\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        n_fold=config[\"n_fold\"]\n",
    "    )\n",
    "\n",
    "    # Model\n",
    "    class LitViTModel(L.LightningModule):\n",
    "        def __init__(self, model_name, lr=2e-5, warmup_epochs=0):\n",
    "            super().__init__()\n",
    "            self.model = ViTForImageClassification.from_pretrained(model_name)\n",
    "            self.criterion = nn.BCEWithLogitsLoss()\n",
    "            self.lr = lr\n",
    "            self.warmup_epochs = warmup_epochs\n",
    "            \n",
    "            self.train_acc = BinaryAccuracy()\n",
    "            self.val_acc = BinaryAccuracy()\n",
    "            self.val_f1 = BinaryF1Score()\n",
    "            self.val_auc = BinaryAUROC()\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.model(x).logits[:, :1]\n",
    "\n",
    "        def training_step(self, batch, batch_idx):\n",
    "            x, y = batch\n",
    "            y = y.float().unsqueeze(1)\n",
    "            logits = self(x)\n",
    "            loss = self.criterion(logits, y)\n",
    "            self.log(\"train_loss\", loss, prog_bar=True)\n",
    "            self.train_acc(torch.sigmoid(logits), y)\n",
    "            self.log(\"train_acc\", self.train_acc, on_step=False, on_epoch=True)\n",
    "            return loss\n",
    "\n",
    "        def validation_step(self, batch, batch_idx):\n",
    "            x, y = batch\n",
    "            y = y.float().unsqueeze(1)\n",
    "            logits = self(x)\n",
    "            loss = self.criterion(logits, y)\n",
    "            \n",
    "            probs = torch.sigmoid(logits)\n",
    "            self.val_acc(probs, y)\n",
    "            self.val_f1(probs, y)\n",
    "            self.val_auc(probs, y)\n",
    "            \n",
    "            self.log(\"val_loss\", loss, prog_bar=True)\n",
    "            self.log(\"val_acc\", self.val_acc, prog_bar=True)\n",
    "            self.log(\"val_f1\", self.val_f1, prog_bar=True)\n",
    "            self.log(\"val_auc\", self.val_auc, prog_bar=True)\n",
    "            return loss\n",
    "\n",
    "        def configure_optimizers(self):\n",
    "            optimizer = torch.optim.Adam(self.parameters(), lr=self.lr)\n",
    "            scheduler = get_cosine_schedule_with_warmup(\n",
    "                optimizer,\n",
    "                num_warmup_steps=self.trainer.estimated_stepping_batches * self.warmup_epochs,\n",
    "                num_training_steps=self.trainer.estimated_stepping_batches * self.trainer.max_epochs\n",
    "            )\n",
    "            return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"step\"}]\n",
    "\n",
    "    # Model\n",
    "    model = LitViTModel(\n",
    "        model_name=config[\"model_name\"],\n",
    "        lr=config[\"lr\"],\n",
    "        warmup_epochs=config[\"warmup_epochs\"]\n",
    "    )\n",
    "    \n",
    "    # Callbacks\n",
    "    early_stop = EarlyStopping(\n",
    "        monitor=\"val_f1\",\n",
    "        patience=3,\n",
    "        mode=\"max\",\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"val_f1\",\n",
    "        mode=\"max\",\n",
    "        save_top_k=1,\n",
    "        filename=\"best-checkpoint\"\n",
    "    )\n",
    "    \n",
    "    # Trainer\n",
    "    trainer = L.Trainer(\n",
    "        logger=mlflow_logger,\n",
    "        callbacks=[early_stop, checkpoint_callback, ray.train.lightning.RayTrainReportCallback()],\n",
    "        max_epochs=config[\"num_epochs\"],\n",
    "        accelerator=\"cpu\",\n",
    "        devices=\"auto\",\n",
    "        enable_progress_bar=True,\n",
    "        log_every_n_steps=10,\n",
    "        strategy=ray.train.lightning.RayDDPStrategy(),\n",
    "        plugins = [ray.train.lightning.RayLightningEnvironment()]\n",
    "    )\n",
    "\n",
    "    # Log parameters\n",
    "    mlflow_logger.log_hyperparams(config)\n",
    "    \n",
    "    # Train\n",
    "    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92ce2ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-13 23:25:50,734\tINFO worker.py:1843 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "2025-04-13 23:25:51,168\tINFO tune.py:616 -- [output] This uses the legacy output and progress reporter, as Jupyter notebooks are not supported by the new engine, yet. For more information, please see https://github.com/ray-project/ray/issues/36949\n",
      "2025-04-13 23:25:51,171\tINFO tensorboardx.py:193 -- pip install \"ray[tune]\" to see TensorBoard files.\n",
      "2025-04-13 23:25:51,171\tWARNING callback.py:136 -- The TensorboardX logger cannot be instantiated because either TensorboardX or one of it's dependencies is not installed. Please make sure you have the latest version of TensorboardX installed: `pip install -U tensorboardx`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ray cluster resources: {'CPU': 6.0, 'memory': 19427491840.0, 'node:127.0.0.1': 1.0, 'object_store_memory': 2147483648.0, 'node:__internal_head__': 1.0}\n",
      "== Status ==\n",
      "Current time: 2025-04-13 23:25:51 (running for 00:00:00.15)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 5.0/6 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2025-04-13_23-25-49_925262_14706/artifacts/2025-04-13_23-25-51/vit_training/driver_artifacts\n",
      "Number of trials: 1/1 (1 PENDING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=14724)\u001b[0m Setting up process group for: env:// [rank=0, world_size=2]\n",
      "\u001b[36m(TorchTrainer pid=14723)\u001b[0m Started distributed worker processes: \n",
      "\u001b[36m(TorchTrainer pid=14723)\u001b[0m - (node_id=159c4f85ac39ea9039d7c8d27ac8eccb694b20959b2a822df3bc0a51, ip=127.0.0.1, pid=14724) world_rank=0, local_rank=0, node_rank=0\n",
      "\u001b[36m(TorchTrainer pid=14723)\u001b[0m - (node_id=159c4f85ac39ea9039d7c8d27ac8eccb694b20959b2a822df3bc0a51, ip=127.0.0.1, pid=14725) world_rank=1, local_rank=1, node_rank=0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-04-13 23:25:56 (running for 00:00:05.25)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 5.0/6 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2025-04-13_23-25-49_925262_14706/artifacts/2025-04-13_23-25-51/vit_training/driver_artifacts\n",
      "Number of trials: 1/1 (1 RUNNING)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(RayTrainWorker pid=14724)\u001b[0m /opt/miniconda3/envs/mlops/lib/python3.12/site-packages/ray/train/lightning/_lightning_utils.py:262: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "\u001b[36m(RayTrainWorker pid=14724)\u001b[0m `get_trial_name` is deprecated because the concept of a `Trial` will soon be removed in Ray Train.Ray Train will no longer assume that it's running within a Ray Tune `Trial` in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(RayTrainWorker pid=14724)\u001b[0m GPU available: True (mps), used: False\n",
      "\u001b[36m(RayTrainWorker pid=14724)\u001b[0m TPU available: False, using: 0 TPU cores\n",
      "\u001b[36m(RayTrainWorker pid=14724)\u001b[0m HPU available: False, using: 0 HPUs\n",
      "\u001b[36m(RayTrainWorker pid=14724)\u001b[0m /opt/miniconda3/envs/mlops/lib/python3.12/site-packages/lightning/pytorch/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\u001b[36m(RayTrainWorker pid=14724)\u001b[0m Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\u001b[36m(RayTrainWorker pid=14724)\u001b[0m /opt/miniconda3/envs/mlops/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(RayTrainWorker pid=14724)\u001b[0m \n",
      "\u001b[36m(RayTrainWorker pid=14724)\u001b[0m   | Name      | Type                      | Params | Mode \n",
      "\u001b[36m(RayTrainWorker pid=14724)\u001b[0m ----------------------------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=14724)\u001b[0m 0 | model     | ViTForImageClassification | 86.6 M | eval \n",
      "\u001b[36m(RayTrainWorker pid=14724)\u001b[0m 1 | criterion | BCEWithLogitsLoss         | 0      | train\n",
      "\u001b[36m(RayTrainWorker pid=14724)\u001b[0m 2 | train_acc | BinaryAccuracy            | 0      | train\n",
      "\u001b[36m(RayTrainWorker pid=14724)\u001b[0m 3 | val_acc   | BinaryAccuracy            | 0      | train\n",
      "\u001b[36m(RayTrainWorker pid=14724)\u001b[0m 4 | val_f1    | BinaryF1Score             | 0      | train\n",
      "\u001b[36m(RayTrainWorker pid=14724)\u001b[0m 5 | val_auc   | BinaryAUROC               | 0      | train\n",
      "\u001b[36m(RayTrainWorker pid=14724)\u001b[0m ----------------------------------------------------------------\n",
      "\u001b[36m(RayTrainWorker pid=14724)\u001b[0m 86.6 M    Trainable params\n",
      "\u001b[36m(RayTrainWorker pid=14724)\u001b[0m 0         Non-trainable params\n",
      "\u001b[36m(RayTrainWorker pid=14724)\u001b[0m 86.6 M    Total params\n",
      "\u001b[36m(RayTrainWorker pid=14724)\u001b[0m 346.271   Total estimated model params size (MB)\n",
      "\u001b[36m(RayTrainWorker pid=14724)\u001b[0m 5         Modules in train mode\n",
      "\u001b[36m(RayTrainWorker pid=14724)\u001b[0m 214       Modules in eval mode\n",
      "\u001b[36m(RayTrainWorker pid=14724)\u001b[0m /opt/miniconda3/envs/mlops/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n",
      "\u001b[36m(RayTrainWorker pid=14724)\u001b[0m /opt/miniconda3/envs/mlops/lib/python3.12/multiprocessing/resource_tracker.py:255: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n",
      "\u001b[36m(RayTrainWorker pid=14724)\u001b[0m   warnings.warn('resource_tracker: There appear to be %d '\n",
      "2025-04-13 23:25:58,143\tERROR tune_controller.py:1331 -- Trial task failed for trial TorchTrainer_2a685_00000\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/ray/air/execution/_internal/event_manager.py\", line 110, in resolve_future\n",
      "    result = ray.get(future)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/ray/_private/worker.py\", line 2782, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/ray/_private/worker.py\", line 929, in get_objects\n",
      "    raise value.as_instanceof_cause()\n",
      "ray.exceptions.RayTaskError(FileNotFoundError): \u001b[36mray::_Inner.train()\u001b[39m (pid=14723, ip=127.0.0.1, actor_id=5bbfe1744e9243b5470ee8a201000000, repr=TorchTrainer)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/ray/tune/trainable/trainable.py\", line 330, in train\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/ray/train/_internal/utils.py\", line 57, in check_for_failure\n",
      "    ray.get(object_ref)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ray.exceptions.RayTaskError(FileNotFoundError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=14725, ip=127.0.0.1, actor_id=c03355d1230e0a3b107742a301000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x1076b2240>)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n",
      "    raise skipped from exception_cause(skipped)\n",
      "  File \"/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/ray/train/_internal/utils.py\", line 176, in discard_return_wrapper\n",
      "    train_func(*args, **kwargs)\n",
      "  File \"/var/folders/n5/73xqm37j4fd6v593l0bz13680000gn/T/ipykernel_14706/1838653843.py\", line 110, in train_func\n",
      "  File \"/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 105, in launch\n",
      "    return function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 138, in run\n",
      "    batch, batch_idx, dataloader_idx = next(data_fetcher)\n",
      "                                       ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/lightning/pytorch/loops/fetchers.py\", line 134, in __next__\n",
      "    batch = super().__next__()\n",
      "            ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/lightning/pytorch/loops/fetchers.py\", line 61, in __next__\n",
      "    batch = next(self.iterator)\n",
      "            ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/lightning/pytorch/utilities/combined_loader.py\", line 341, in __next__\n",
      "    out = next(self._iterator)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/lightning/pytorch/utilities/combined_loader.py\", line 142, in __next__\n",
      "    out = next(self.iterators[0])\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 735, in __next__\n",
      "    data = self._next_data()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 791, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 50, in fetch\n",
      "    data = self.dataset.__getitems__(possibly_batched_index)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/torch/utils/data/dataset.py\", line 416, in __getitems__\n",
      "    return [self.dataset[self.indices[idx]] for idx in indices]\n",
      "            ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/var/folders/n5/73xqm37j4fd6v593l0bz13680000gn/T/ipykernel_14706/3537786605.py\", line 12, in __getitem__\n",
      "  File \"/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/PIL/Image.py\", line 3469, in open\n",
      "    fp = builtins.open(filename, \"rb\")\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: '/private/tmp/ray/session_2025-04-13_23-25-49_925262_14706/artifacts/2025-04-13_23-25-51/vit_training/working_dirs/TorchTrainer_2a685_00000_0_2025-04-13_23-25-51/train_data/041c36d9269146cdb88e7526e3b91651.jpg'\n",
      "2025-04-13 23:25:58,192\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/Users/anshsarkar/ray_results/vit_training' in 0.0465s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]🏃 View run fold-0 at: http://127.0.0.1:8080/#/experiments/634370650965998394/runs/7105ac036d394b43b96b43e99f8bca44\n",
      "\u001b[36m(RayTrainWorker pid=14724)\u001b[0m 🧪 View experiment at: http://127.0.0.1:8080/#/experiments/634370650965998394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-13 23:25:58,195\tERROR tune.py:1037 -- Trials did not complete: [TorchTrainer_2a685_00000]\n",
      "2025-04-13 23:25:58,195\tINFO tune.py:1041 -- Total run time: 7.03 seconds (6.97 seconds for the tuning loop).\n",
      "\u001b[36m(RayTrainWorker pid=14725)\u001b[0m /opt/miniconda3/envs/mlops/lib/python3.12/site-packages/ray/train/lightning/_lightning_utils.py:262: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "\u001b[36m(RayTrainWorker pid=14725)\u001b[0m `get_trial_name` is deprecated because the concept of a `Trial` will soon be removed in Ray Train.Ray Train will no longer assume that it's running within a Ray Tune `Trial` in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(RayTrainWorker pid=14725)\u001b[0m /opt/miniconda3/envs/mlops/lib/python3.12/multiprocessing/resource_tracker.py:255: UserWarning: resource_tracker: There appear to be 1 leaked semaphore objects to clean up at shutdown\n",
      "\u001b[36m(RayTrainWorker pid=14725)\u001b[0m   warnings.warn('resource_tracker: There appear to be %d '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Status ==\n",
      "Current time: 2025-04-13 23:25:58 (running for 00:00:07.02)\n",
      "Using FIFO scheduling algorithm.\n",
      "Logical resource usage: 5.0/6 CPUs, 0/0 GPUs\n",
      "Result logdir: /tmp/ray/session_2025-04-13_23-25-49_925262_14706/artifacts/2025-04-13_23-25-51/vit_training/driver_artifacts\n",
      "Number of trials: 1/1 (1 ERROR)\n",
      "Number of errored trials: 1\n",
      "+--------------------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "| Trial name               |   # failures | error file                                                                                                                                                             |\n",
      "|--------------------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
      "| TorchTrainer_2a685_00000 |            1 | /tmp/ray/session_2025-04-13_23-25-49_925262_14706/artifacts/2025-04-13_23-25-51/vit_training/driver_artifacts/TorchTrainer_2a685_00000_0_2025-04-13_23-25-51/error.txt |\n",
      "+--------------------------+--------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "ename": "TrainingFailedError",
     "evalue": "The Ray Train run failed. Please inspect the previous error messages for a cause. After fixing the issue (assuming that the error is not caused by your own application logic, but rather an error such as OOM), you can restart the run from scratch or continue this run.\nTo continue this run, you can use: `trainer = TorchTrainer.restore(\"/Users/anshsarkar/ray_results/vit_training\")`.\nTo start a new run that will retry on training failures, set `train.RunConfig(failure_config=train.FailureConfig(max_failures))` in the Trainer's `run_config` with `max_failures > 0`, or `max_failures = -1` for unlimited retries.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRayTaskError(FileNotFoundError)\u001b[39m           Traceback (most recent call last)",
      "\u001b[31mRayTaskError(FileNotFoundError)\u001b[39m: \u001b[36mray::_Inner.train()\u001b[39m (pid=14723, ip=127.0.0.1, actor_id=5bbfe1744e9243b5470ee8a201000000, repr=TorchTrainer)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/ray/tune/trainable/trainable.py\", line 330, in train\n    raise skipped from exception_cause(skipped)\n  File \"/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/ray/train/_internal/utils.py\", line 57, in check_for_failure\n    ray.get(object_ref)\n           ^^^^^^^^^^^^^^^^^^^\n           ^^^^^^^^^^^^^^^^^^^^^\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nray.exceptions.RayTaskError(FileNotFoundError): \u001b[36mray::_RayTrainWorker__execute.get_next()\u001b[39m (pid=14725, ip=127.0.0.1, actor_id=c03355d1230e0a3b107742a301000000, repr=<ray.train._internal.worker_group.RayTrainWorker object at 0x1076b2240>)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/ray/train/_internal/worker_group.py\", line 33, in __execute\n    raise skipped from exception_cause(skipped)\n  File \"/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/ray/train/_internal/utils.py\", line 176, in discard_return_wrapper\n    train_func(*args, **kwargs)\n  File \"/var/folders/n5/73xqm37j4fd6v593l0bz13680000gn/T/ipykernel_14706/1838653843.py\", line 110, in train_func\n  File \"/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py\", line 561, in fit\n    call._call_and_handle_interrupt(\n  File \"/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py\", line 47, in _call_and_handle_interrupt\n    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/lightning/pytorch/strategies/launchers/subprocess_script.py\", line 105, in launch\n    return function(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py\", line 599, in _fit_impl\n    self._run(model, ckpt_path=ckpt_path)\n  File \"/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py\", line 1012, in _run\n    results = self._run_stage()\n              ^^^^^^^^^^^^^^^^^\n  File \"/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py\", line 1054, in _run_stage\n    self._run_sanity_check()\n  File \"/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py\", line 1083, in _run_sanity_check\n    val_loop.run()\n  File \"/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/lightning/pytorch/loops/utilities.py\", line 179, in _decorator\n    return loop_run(self, *args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/lightning/pytorch/loops/evaluation_loop.py\", line 138, in run\n    batch, batch_idx, dataloader_idx = next(data_fetcher)\n                                       ^^^^^^^^^^^^^^^^^^\n  File \"/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/lightning/pytorch/loops/fetchers.py\", line 134, in __next__\n    batch = super().__next__()\n            ^^^^^^^^^^^^^^^^^^\n  File \"/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/lightning/pytorch/loops/fetchers.py\", line 61, in __next__\n    batch = next(self.iterator)\n            ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/lightning/pytorch/utilities/combined_loader.py\", line 341, in __next__\n    out = next(self._iterator)\n          ^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/lightning/pytorch/utilities/combined_loader.py\", line 142, in __next__\n    out = next(self.iterators[0])\n          ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 735, in __next__\n    data = self._next_data()\n           ^^^^^^^^^^^^^^^^^\n  File \"/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 791, in _next_data\n    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 50, in fetch\n    data = self.dataset.__getitems__(possibly_batched_index)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/torch/utils/data/dataset.py\", line 416, in __getitems__\n    return [self.dataset[self.indices[idx]] for idx in indices]\n            ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^\n  File \"/var/folders/n5/73xqm37j4fd6v593l0bz13680000gn/T/ipykernel_14706/3537786605.py\", line 12, in __getitem__\n  File \"/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/PIL/Image.py\", line 3469, in open\n    fp = builtins.open(filename, \"rb\")\n         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nFileNotFoundError: [Errno 2] No such file or directory: '/private/tmp/ray/session_2025-04-13_23-25-49_925262_14706/artifacts/2025-04-13_23-25-51/vit_training/working_dirs/TorchTrainer_2a685_00000_0_2025-04-13_23-25-51/train_data/041c36d9269146cdb88e7526e3b91651.jpg'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mTrainingFailedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 44\u001b[39m\n\u001b[32m     32\u001b[39m     run_config = RunConfig(\n\u001b[32m     33\u001b[39m         \u001b[38;5;66;03m# storage_path=\"/tmp/ray_results\",  # Local storage path\u001b[39;00m\n\u001b[32m     34\u001b[39m         name=\u001b[33m\"\u001b[39m\u001b[33mvit_training\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     35\u001b[39m     )\n\u001b[32m     37\u001b[39m     trainer = TorchTrainer(\n\u001b[32m     38\u001b[39m         train_func,\n\u001b[32m     39\u001b[39m         train_loop_config=config,\n\u001b[32m     40\u001b[39m         scaling_config=scaling_config,\n\u001b[32m     41\u001b[39m         run_config=run_config\n\u001b[32m     42\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     result = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining completed successfully.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/mlops/lib/python3.12/site-packages/ray/train/base_trainer.py:720\u001b[39m, in \u001b[36mBaseTrainer.fit\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    716\u001b[39m result = result_grid[\u001b[32m0\u001b[39m]\n\u001b[32m    717\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result.error:\n\u001b[32m    718\u001b[39m     \u001b[38;5;66;03m# Raise trainable errors to the user with a message to restore\u001b[39;00m\n\u001b[32m    719\u001b[39m     \u001b[38;5;66;03m# or configure `FailureConfig` in a new run.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m720\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m TrainingFailedError(\n\u001b[32m    721\u001b[39m         \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join([restore_msg, TrainingFailedError._FAILURE_CONFIG_MSG])\n\u001b[32m    722\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mresult\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01merror\u001b[39;00m\n\u001b[32m    723\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[31mTrainingFailedError\u001b[39m: The Ray Train run failed. Please inspect the previous error messages for a cause. After fixing the issue (assuming that the error is not caused by your own application logic, but rather an error such as OOM), you can restart the run from scratch or continue this run.\nTo continue this run, you can use: `trainer = TorchTrainer.restore(\"/Users/anshsarkar/ray_results/vit_training\")`.\nTo start a new run that will retry on training failures, set `train.RunConfig(failure_config=train.FailureConfig(max_failures))` in the Trainer's `run_config` with `max_failures > 0`, or `max_failures = -1` for unlimited retries."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Configuration\n",
    "    # ray.init(\n",
    "    #     num_cpus=6,  # Adjust based on your Mac's CPU cores\n",
    "    #     include_dashboard=True,  # Disable dashboard to reduce overhead\n",
    "    #     ignore_reinit_error=True\n",
    "    # )\n",
    "\n",
    "    # print(\"Ray cluster resources:\", ray.cluster_resources())\n",
    "\n",
    "    config = {\n",
    "        \"labels\": pd.read_csv(\"./train.csv\").iloc[:, 1:].copy(),\n",
    "        \"img_dir\": \"./\",\n",
    "        \"model_name\": \"google/vit-base-patch16-224\",\n",
    "        \"img_size\": (224, 224),\n",
    "        \"batch_size\": 32,  # Reduced for local execution\n",
    "        \"lr\": 2e-5,\n",
    "        \"num_epochs\": 10,\n",
    "        \"warmup_epochs\": 0,\n",
    "        \"n_fold\": 0,\n",
    "        \"num_workers\": 2  # Number of parallel training workers\n",
    "    }\n",
    "\n",
    "    try:\n",
    "\n",
    "        scaling_config = ScalingConfig(\n",
    "            num_workers=config.get(\"num_workers\", 1),  # Using 2 workers for local Mac\n",
    "            use_gpu=False,  # Mac typically doesn't have supported GPUs for PyTorch\n",
    "            resources_per_worker={\"CPU\": 2}  # Allocate 2 CPUs per worker\n",
    "        )\n",
    "        \n",
    "        run_config = RunConfig(\n",
    "            # storage_path=\"/tmp/ray_results\",  # Local storage path\n",
    "            name=\"vit_training\"\n",
    "        )\n",
    "        \n",
    "        trainer = TorchTrainer(\n",
    "            train_func,\n",
    "            train_loop_config=config,\n",
    "            scaling_config=scaling_config,\n",
    "            run_config=run_config\n",
    "        )\n",
    "        \n",
    "        result = trainer.fit()\n",
    "        print(\"Training completed successfully.\")\n",
    "    finally:\n",
    "        # ray.shutdown()\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9563e52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4fe58b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
